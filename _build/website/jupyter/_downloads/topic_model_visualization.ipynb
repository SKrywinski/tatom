{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='index-0'></a>\n",
    "\n",
    "<a id='topic-model-visualization'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing topic models\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Unlike R or Octave/Matlab, Python has no native graphing or\n",
    "visualization library. The most widely used library for visualization is\n",
    "[matplotlib](http://matplotlib.org) and its functions mimic those found in\n",
    "Octave/Matlab (see [For those new to Matplotlib](getting_started.html#getting-started-matplotlib))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing topic shares\n",
    "\n",
    "When using topic models to explore text collections, we are typically interested\n",
    "in examining texts in terms of their constituent topics (instead of word\n",
    "frequencies).  Because the number of topics is so much smaller than the number\n",
    "of unique vocabulary elements (say, 20 versus 20,000), a range of data\n",
    "visualization methods become available. The visualization techniques discussed\n",
    "below are not specific to topic models per se but rather fall into a more\n",
    "general category for techniques for visualizing count data.\n",
    "\n",
    "The first way of visualizing the topic proportions associated with documents in\n",
    "a corpus uses a “stacked bar chart”.  This visualization tends to be used\n",
    "when the number of topics is small. For example, a stacked\n",
    "bar chart is found in a paper employing LDA *avant la lettre* on [allele](https://en.wikipedia.org/wiki/Allele) frequency data\n",
    "[[RPW+02]](references.html#rosenberg-genetic-2002). <sup><a href=#fnpritchard id=fnpritchard-link>[1]</a></sup>\n",
    "\n",
    "<img src=\"_static/rosenberg-et-al-2002-figure.png\" alt=\"Figure from Rosenberg et al. (2002), using methods described in Pritchard et al. (2000)\" style=\"width:60%;height:60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked bar chart\n",
    "\n",
    "The idea underlying the stacked bar chart is that each text has some proportion\n",
    "of its words associated with each topic. Because the model assumes that every\n",
    "word is associated with some topic, these proportions must add up to one. For\n",
    "example, in a three topic model, text number 1 might have 50% of its words\n",
    "associated with topic 1, 25% with topic 2, and 25% with topic 3. The stacked bar\n",
    "chart represents each document as a bar broken into colored segments matching\n",
    "the associated proportions of each topic. The stacked bar chart below expresses\n",
    "the topic proportions found in the six novels in the `austen-brontë` corpus:\n",
    "\n",
    "<img src=\"_static/plot_doctopic_stacked_bar.png\" alt=\"Topic shares in six novels\" style=\"width:60%;height:60%\">\n",
    "\n",
    "  \n",
    "If we look ahead to the [top 10 words associated with each topic](#visualizing-topic-word), we find that topics 0 and 1 are associated with words\n",
    "we anticipate finding in Austen (‘emma’, ‘elizabeth’, ‘darcy’, ‘sister’). Topic\n",
    "4 is connected with *Jane Eyre* (‘rochester’, ‘house’).\n",
    "\n",
    "This method of visualization works well when there are a small number of topics.\n",
    "With more than a handful of topics, the proportion associated with\n",
    "each topic tends to be difficult to distinguish. This is a limitation of the\n",
    "stacked bar chart. On the other hand, the visualization clearly communicates the\n",
    "idea that a document is made up of topic proportions that, taken together,\n",
    "account for all the words in a document.\n",
    "\n",
    "To reproduce the stacked bar chart shown above we first need to model the\n",
    "`austen-brontë` corpus using a five-topic topic model. We will use the\n",
    "procedure described in [Topic modeling with MALLET](topic_model_mallet.html#topic-model-mallet). First\n",
    "we run MALLET using default parameters. As we will be analyzing more precisely\n",
    "the association of words with topics, we will specify that the counts of\n",
    "word-topic assignments be saved with the command-line option `--word-topic counts\n",
    "/tmp/word-topic.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide-output": false
   },
   "source": [
    "```bash\n",
    "mallet-2.0.7/bin/mallet import-dir --input data/austen-brontë-split/ --output /tmp/topic-input.mallet --keep-sequence --remove-stopwords\n",
    "mallet-2.0.7/bin/mallet train-topics --input /tmp/topic-input.mallet --num-topics 5 --output-doc-topics /tmp/doc-topics.txt --output-topic-keys /tmp/topic-keys.txt --word-topic-counts-file /tmp/word-topic.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then gather the output into a document-topic matrix of topic shares, stored\n",
    "in the variable `doctopic`. The names of the novels are stored in the variable\n",
    "`docnames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "docnames\n",
    "doctopic.shape\n",
    "doctopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the stacked bar chart we layer individual bar charts on top of each\n",
    "other. Recall that a single (unstacked) bar chart can be created in matplotlib\n",
    "with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N, K = doctopic.shape\n",
    "ind = np.arange(N)  # points on the x-axis\n",
    "width = 0.5\n",
    "\n",
    "plt.bar(ind, doctopic[:,0], width=width)\n",
    "plt.xticks(ind + width/2, docnames)  # put labels in the center\n",
    "\n",
    "@savefig plot_example_bar.png width=7in\n",
    "plt.title('Share of Topic #0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**\n",
    ">\n",
    ">The [matplotlib examples page](http://matplotlib.org/examples/index.html) describes\n",
    "how to produce various types of graphs using matplotlib.\n",
    "\n",
    "To layer the bar charts, we plot each bar chart one by one, adjusting the\n",
    "starting point (`bottom` is the parameter) so that the charts do not overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# See: http://matplotlib.org/examples/pylab_examples/bar_stacked.html\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N, K = doctopic.shape  # N documents, K topics\n",
    "ind = np.arange(N)  # the x-axis locations for the novels\n",
    "width = 0.5  # the width of the bars\n",
    "plots = []\n",
    "height_cumulative = np.zeros(N)\n",
    "for k in range(K):\n",
    "    color = plt.cm.coolwarm(k/K, 1)\n",
    "    if k == 0:\n",
    "        # first plot\n",
    "        p = plt.bar(ind, doctopic[:, k], width, color=color)\n",
    "    else:\n",
    "        p = plt.bar(ind, doctopic[:, k], width, bottom=height_cumulative, color=color)\n",
    "    height_cumulative += doctopic[:, k]\n",
    "    plots.append(p)\n",
    "plt.ylim((0, 1))  # proportions sum to 1, so the height of the stacked bars is 1\n",
    "plt.ylabel('Topics')\n",
    "plt.title('Topics in novels')\n",
    "plt.xticks(ind+width/2, docnames)\n",
    "plt.yticks(np.arange(0, 1, 10))\n",
    "topic_labels = ['Topic #{}'.format(k) for k in range(K)]\n",
    "# see http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.legend for details\n",
    "# on making a legend in matplotlib\n",
    "plt.legend([p[0] for p in plots], topic_labels)\n",
    "\n",
    "@savefig plot_doctopic_stacked_bar.png width=7in\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap\n",
    "\n",
    "Another useful visualization of topic shares is the heatmap. The matplotlib\n",
    "function we need is `pcolor` (“psuedocolor plot”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Ref: http://nbviewer.ipython.org/5427209\n",
    "# Ref: http://code.activestate.com/recipes/578175-hierarchical-clustering-heatmap-python/\n",
    "\n",
    "plt.pcolor(doctopic, norm=None, cmap='Blues')\n",
    "\n",
    "# put the major ticks at the middle of each cell\n",
    "# the trailing semicolon ';' suppresses output\n",
    "plt.yticks(np.arange(doctopic.shape[0])+0.5, docnames);\n",
    "plt.xticks(np.arange(doctopic.shape[1])+0.5, topic_labels);\n",
    "\n",
    "# flip the y-axis so the texts are in the order we anticipate (Austen first, then Brontë)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# rotate the ticks on the x-axis\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# add a legend\n",
    "plt.colorbar(cmap='Blues')\n",
    "\n",
    "plt.tight_layout()  # fixes margins\n",
    "\n",
    "@savefig plot_doctopic_heatmap.png width=7in\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that topics 0 and 1 are strongly associated with the Austen\n",
    "novels, whereas words assigned to topic 3 predominate in novels written by\n",
    "Brontë.\n",
    "\n",
    "\n",
    "<a id='visualizing-topic-word'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing topic-word associations\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">For the following visualizations we will need the counts of\n",
    "word-to-topic assignments. These are stored in a file specified by the\n",
    "MALLET option `--word-topic-counts-file`. The following assumes that the\n",
    "file containing the counts is available at `/tmp/word-topic.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table\n",
    "\n",
    "We have already seen a simple way of “visualizing” the associations between\n",
    "topics and words: lists of the top words associated with each topic are often\n",
    "all that is needed when the corpus is large and the inferred topics make sense\n",
    "in light of prior knowledge of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "with open('/tmp/topic-keys.txt') as input:\n",
    "    topic_keys_lines = input.readlines()\n",
    "topic_words = []\n",
    "for line in topic_keys_lines:\n",
    "    _, _, words = line.split('\\t')  # tab-separated\n",
    "    words = words.rstrip().split(' ')  # remove the trailing '\\n'\n",
    "    topic_words.append(words)\n",
    "\n",
    "for t in range(len(topic_words)):\n",
    "    print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:15])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing words according to strength of association with a topic\n",
    "\n",
    "Each topic is a distribution over the vocabulary of words found in the corpus.\n",
    "The top words (saved via the `--topic-keys` option) are those words most\n",
    "likely to be found in each topic. Displaying the top words does not, however,\n",
    "convey any information about the probability of finding a top word assigned to\n",
    "a specific topic. This is often critical information.  (For those in the process\n",
    "of learning about discrete probability distributions, we may think of this\n",
    "probability as the ‘strength of association’ between a word and a topic.) For\n",
    "example, the top five words of topic 1 may be much more strongly associated with\n",
    "the topic than the top five words of topic 2. In some cases this can be extreme,\n",
    "the tenth top word for topic 1 may hardly ever be associated with topic 1,\n",
    "whereas the tenth top word for topic 2 may be assigned frequently to topic 2.\n",
    "Moreover, if most of the words in the vocabulary are equally associated with\n",
    "a topic then identifying the “top 10 words” is misleading.\n",
    "\n",
    "To estimate the strength of association between a given word and a given topic\n",
    "we need the information saved in the file specified by the MALLET option\n",
    "`--word-topic-counts-file`, here `/tmp/word-topic.txt`. The first few lines\n",
    "of this file look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "0 delightful 0:86\n",
    "1 thing 0:801 3:1\n",
    "2 daughter 1:278\n",
    "3 married 1:251"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line indicates that the word ‘delightful’ is assigned to topic 0 86\n",
    "times. The second line tells us that the word ‘thing’ is associated with topic\n",
    "0 801 times and to topic 3 once.\n",
    "\n",
    "In order to make systematic use of this information we need to parse this file\n",
    "into a matrix of counts. We do this much in the same manner as we parsed\n",
    "`doc-topics.txt` into a matrix of document-topic shares. The following\n",
    "procedure should be familiar by now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "num_topics = 5\n",
    "\n",
    "mallet_vocab = []\n",
    "word_topic_counts = []\n",
    "\n",
    "with open(\"/tmp/word-topic.txt\") as f:\n",
    "    for line in f:\n",
    "        _, word, *topic_count_pairs = line.rstrip().split(' ')\n",
    "        # turn topic_count_pairs from a string like \"0:30 1:20\" to a sequence of pairs [(0, 30), (1, 20)]\n",
    "        topic_count_pairs = [pair.split(':') for pair in topic_count_pairs]\n",
    "        mallet_vocab.append(word)\n",
    "        # allocate an 'empty' array of zeros to store the counts for this word\n",
    "        counts = np.zeros(num_topics)\n",
    "        for topic, count in topic_count_pairs:\n",
    "            counts[int(topic)] = int(count)\n",
    "        word_topic_counts.append(counts)\n",
    "word_topic = np.array(word_topic_counts)\n",
    "\n",
    "word_topic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since topics will be assigned differing numbers of words we need to normalize\n",
    "this matrix of counts in much the same way as we did for the matrix of\n",
    "document-term counts (see [Working with text](working_with_text.html#working-with-text)). For example, while the word\n",
    "“delightful” may be assigned to topic 0 86 times, there may be many other words\n",
    "that are also assigned to topic 0 a similar or greater number of times. In order\n",
    "to measure the strength of association between “delightful” and topic 0 we need\n",
    "to know how likely it is to be associated with topic 0 relative to all other\n",
    "words.  We do this by calculating the proportion of words assigned to topic\n",
    "0 that are the word “delightful”. Doing this for each word and each topic in\n",
    "turn will turn our matrix of word-topic counts into a matrix of word-topic\n",
    "proportions. For example, a value of 0.5 in the matrix at row 5 and column\n",
    "0 indicates that the specified word type (`mallet_vocab[5]`) accounts for 50\n",
    "percent of all words assigned to topic 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# np.sum(word_topic, axis=0) sums across rows, so it yields totals of words assigned to topics\n",
    "word_topic = word_topic / np.sum(word_topic, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assemble a list of each topic’s top words along with a value that\n",
    "captures the strength of association with that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "mallet_vocab = np.array(mallet_vocab)  # convert vocab from a list to an array so we can use NumPy operations on it\n",
    "for t in range(num_topics):\n",
    "    top_words_idx = np.argsort(word_topic[:,t])[::-1]  # descending order\n",
    "    top_words_idx = top_words_idx[:num_top_words]\n",
    "    top_words = mallet_vocab[top_words_idx]\n",
    "    top_words_shares = word_topic[top_words_idx, t]\n",
    "    print(\"Topic #{}:\".format(t))\n",
    "    for word, share in zip(top_words, top_words_shares):\n",
    "        print(\"{} : {}\".format(np.round(share, 3), word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to visualize this information is to size each word in proportion to its\n",
    "share of words associated with each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "num_top_words = 10\n",
    "fontsize_base = 70 / np.max(word_topic) # font size for word with largest share in corpus\n",
    "for t in range(num_topics):\n",
    "    plt.subplot(1, num_topics, t + 1)  # plot numbering starts with 1\n",
    "    plt.ylim(0, num_top_words + 0.5)  # stretch the y-axis to accommodate the words\n",
    "    plt.xticks([])  # remove x-axis markings ('ticks')\n",
    "    plt.yticks([]) # remove y-axis markings ('ticks')\n",
    "    plt.title('Topic #{}'.format(t))\n",
    "    top_words_idx = np.argsort(word_topic[:,t])[::-1]  # descending order\n",
    "    top_words_idx = top_words_idx[:num_top_words]\n",
    "    top_words = mallet_vocab[top_words_idx]\n",
    "    top_words_shares = word_topic[top_words_idx, t]\n",
    "    for i, (word, share) in enumerate(zip(top_words, top_words_shares)):\n",
    "        plt.text(0.3, num_top_words-i-0.5, word, fontsize=fontsize_base*share)\n",
    "\n",
    "@savefig plot_word_topic.png width=7in\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that topic 3 is much more concentrated on the words shown above\n",
    "whereas topic 0 is much more diffuse (or uniform). Another way to appreciate\n",
    "this is to calculate the number of word *types* associated with each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "np.sum(word_topic > 0, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that a greater diversity of vocabulary items are associated with\n",
    "topic 0 (likely many of the French words that appear only in Brontë’s *The\n",
    "Professor*) than with topic 3.\n",
    "\n",
    "<p><a id=fnpritchard href=#fnpritchard-link><strong>[1]</strong></a> The topic model now familiar as LDA was independently\n",
    "discovered and published in 2000 by Pritchard et al.\n",
    "[[PSD00]](references.html#pritchard-inference-2000)."
   ]
  }
 ],
 "metadata": {
  "date": 1577189966.0525048,
  "filename": "topic_model_visualization.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Visualizing topic models"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}