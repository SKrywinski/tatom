{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='index-0'></a>\n",
    "\n",
    "<a id='case-study-racine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study: Racine’s early and late tragedies <sup><a href=#fn-authors id=fn-authors-link>[1]</a></sup>\n",
    "\n",
    "This tutorial describes two stages of an analysis of a collection of French\n",
    "tragedies: (1) exploratory data analysis and (2) a closer quantitative reading\n",
    "of an aspect of interest in the corpus: [Jean Racine’s](https://en.wikipedia.org/wiki/Jean_Racine) early and late plays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus: sixty tragedies\n",
    "\n",
    "The corpus being explored is a collection of 60 French tragedies gathered from\n",
    "Paul Fièvre’s [„Théâtre classique” collection of French seventeenth and\n",
    "eighteenth-century plays](http://www.theatre-classique.fr) (available in\n",
    "[Datasets](datasets.html#datasets)).\n",
    "\n",
    "Following the maxim “know your data”, we first want to gain a rough sense of the\n",
    "length (in words) of the texts in a corpus. Are we dealing with 100,000 word\n",
    "tomes or 1,000 word newspaper articles? Even though we know we are dealing with\n",
    "French tragedies inspecting text lengths is always worthwhile. For example,\n",
    "checking the length of the texts in a corpus is an easy way to spot problems\n",
    "such as incomplete texts. In the present case, we might also consider checking\n",
    "the number of lines ([alexandrines](https://en.wikipedia.org/wiki/Alexandrine)) as this is the standard measure\n",
    "for compositions in verse.  Regardless of what unit is used, the easiest way\n",
    "to visualize the distribution of lengths in a corpus is a histogram. In\n",
    "order to create a histogram we need first to calculate the length of all the\n",
    "texts in our corpus. If we have a document-term matrix available, the length in\n",
    "words may be calculated by summing the rows of the document-term matrix, as each\n",
    "row corresponds to one document.\n",
    "\n",
    "Now that we have the texts’ lengths stored in the array `lengths` we can\n",
    "create a histogram with the matplotlib function `hist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(lengths)\n",
    "@savefig case_study_voltaire_hist.png width=8in\n",
    "plt.title(\"Play length in words\")\n",
    "\n",
    "# find the longest and the shortest texts\n",
    "(np.max(lengths), filenames[np.argmax(lengths)])\n",
    "(np.min(lengths), filenames[np.argmin(lengths)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning to the contents of the corpus, one way to explore a collection is to\n",
    "plot the distances between texts. We have already seen how to do this in\n",
    "[Working with text](working_with_text.html#working-with-text). As we have not normalized our texts by play\n",
    "length—and the histogram of lengths shows us that there is considerable\n",
    "variation—cosine distance is an appropriate choice for a measure of distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# we need distance, not similarity\n",
    "dist = 1 - cosine_similarity(dtm)\n",
    "\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "# create very short names for plotting\n",
    "# filenames have form: Voltaire_TR-V-1724-Mariamne.txt\n",
    "names = []\n",
    "authors = []\n",
    "for fn in filenames:\n",
    "    author = fn.split('_')[0]\n",
    "    year = fn.split('-')[2]\n",
    "    authors.append(author)\n",
    "    names.append(author + year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11.3, 7))  # use a bigger canvas than usual\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "authors_unique = sorted(set(authors))\n",
    "colors = [authors_unique.index(a) for a in authors]\n",
    "plt.scatter(xs, ys, c=colors, cmap='spring')\n",
    "for x, y, name in zip(xs, ys, names):\n",
    "    plt.text(x, y, name, alpha=0.5, fontsize=10)\n",
    "\n",
    "@savefig plot_voltaire_mds.png width=8in\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of plot can get overwhelming. A dendrogram plot offers an alternative\n",
    "means of representing the same information (i.e., the distance between texts).\n",
    "It is important, however, not to take the implied hierarchy too seriously.\n",
    "While two texts paired together in the dendrogram are indeed nearest neighbors\n",
    "in terms of distance, there are a variety of methods of hierarchical clustering\n",
    "each often yielding different hierarchies. Right now we are interested in the\n",
    "dendrogram as a convenient summary of the multi-dimensional scaling plot shown\n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist)\n",
    "\n",
    "plt.figure(figsize=(11.3, 11.3))  # we need a tall figure\n",
    "# match dendrogram to that returned by R's hclust()\n",
    "dendrogram(linkage_matrix, orientation=\"right\", labels=names, leaf_font_size=5);\n",
    "@savefig plot_voltaire_ward_dendrogram.png width=8in\n",
    "plt.tight_layout()  # fixes margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should come as no surprise that texts by the same author tend to be adjacent\n",
    "in the dendrogram. It is well documented that authors frequently leave stylistic\n",
    "“signatures” that are detectable at the level of word frequency. <sup><a href=#fn-authorship-attribution id=fn-authorship-attribution-link>[2]</a></sup>\n",
    "There are, however, a number of plays that do not follow the rule and are paired\n",
    "with texts by other writers. A number of these plays are attributed to Racine\n",
    "and it is to these plays we will turn our attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Racine’s atypical plays\n",
    "\n",
    "Racine’s atypical plays are easiest to detect on the dendrogram. They include:\n",
    "\n",
    "- [La Thébaïde](https://fr.wikipedia.org/wiki/La_Th%C3%A9ba%C3%AFde_%28Racine%29) (1664)  \n",
    "- [Phèdre](https://fr.wikipedia.org/wiki/Ph%C3%A8dre_%28Racine%29) (1677)  \n",
    "- [Esther](https://fr.wikipedia.org/wiki/Esther_%28Racine%29) (1689)  \n",
    "- [Athalie](https://fr.wikipedia.org/wiki/Athalie_%28Racine%29) (1691)  \n",
    "\n",
    "\n",
    "Considering these outliers in the context of the chronology of Racine’s works as\n",
    "a whole is helpful. These plays include the first and the final three plays\n",
    "written by Racine. To display this chronology visually we may use a raster\n",
    "graph, coloring the outliers a distinct color. (This visualization has the added\n",
    "benefit of showing the nearly ten year gap between plays in the 1680s.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@suppress\n",
    "plt.figure(figsize=(11.3, 5))  # reset figure size\n",
    "\n",
    "outliers = [1664, 1677, 1689, 1691]\n",
    "racine_years = []\n",
    "for fn in filenames:\n",
    "    author = fn.split('_')[0]\n",
    "    year = int(fn.split('-')[2])\n",
    "    if author == \"Racine\":\n",
    "        racine_years.append(year)\n",
    "racine_years = np.array(racine_years)\n",
    "colors = []\n",
    "for year in racine_years:\n",
    "    colors.append('orange' if year in outliers else 'cyan')\n",
    "plt.vlines(racine_years, 0, 1, linewidth=3, color=colors)\n",
    "\n",
    "plt.title(\"Year of publication of Racine's plays\")\n",
    "# gca() stands for get current axes. Axes are a matplotlib primitive.\n",
    "# See http://matplotlib.org/users/pyplot_tutorial.html#working-with-multiple-figures-and-axes\n",
    "ax = plt.gca()\n",
    "@savefig plot_racine_voltaire_rastergram.png width=8in\n",
    "ax.yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A provisional explanation for why the late plays stand out might draw on\n",
    "Racine’s religious turn. In 1679 he married Catherine de\n",
    "Romanet and his [Jansenism](https://en.wikipedia.org/wiki/Jansenism) grew more\n",
    "pronounced. The title *Esther* refers to the biblical book of the same name and\n",
    "*Athalie*, Racine’s final play, stages events from the Bible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "A useful way to explore the atypical plays further is to directly compare the\n",
    "outliers with a fictitious “average” Racine tragedy. To do this we will first\n",
    "decompose our documents into an ersatz “topic model” using non-negative matrix\n",
    "factorization and then we will average the topic shares of the “normal” Racine\n",
    "plays and compare those shares with the shares of the atypical plays.\n",
    "\n",
    "We will fit the NMF model using the corpus of tragedies split into approximately\n",
    "1,000-word sections. Recall that before feeding your document-term matrix into\n",
    "NMF it is helpful to normalize each document by length. Here we will\n",
    "normalize and, additionally, use tf-idf weighting as the invocation is simple:\n",
    "we use `TfidfVectorizer` instead of `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/french-tragedy-split/'\n",
    "\n",
    "filenames = np.array(sorted(os.listdir(data_dir)))\n",
    "filenames_with_path = [os.path.join(data_dir, fn) for fn in filenames]\n",
    "\n",
    "# check the first few filenames\n",
    "filenames_with_path[0:4]\n",
    "\n",
    "vectorizer = text.TfidfVectorizer(input='filename', min_df=15)\n",
    "dtm = vectorizer.fit_transform(filenames_with_path)\n",
    "dtm = dtm.toarray()\n",
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# fit NMF model\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "num_topics = 15\n",
    "\n",
    "clf = decomposition.NMF(n_components=num_topics, random_state=1)\n",
    "\n",
    "# this next step may take some time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide-output": false
   },
   "source": [
    "```python\n",
    "doctopic_chunks = clf.fit_transform(dtm)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to interpret and visualize the NMF components in a manner analogous to\n",
    "LDA topic proportions, we will scale the document-component matrix such that\n",
    "the component values associated with each document sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# to avoid division by 0 (not a problem with LDA) we add a tiny value to each cell.\n",
    "doctopic_chunks += 1e-6  # 1e-6 is the same as 0.000001\n",
    "doctopic_chunks = doctopic_chunks / np.sum(doctopic_chunks, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in [previous sections](topic_model_mallet.html#topic-model-mallet), we will aggregate\n",
    "the text sections associated with a single play together and average their topic\n",
    "proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import operator\n",
    "\n",
    "# Play sections have filenames like: Racine_TR-V-1677-Phedre0000.txt. We can split\n",
    "# the last part \"0000.txt\" off using string slicing since we know that the part of\n",
    "# the filename we do not want is always 8 characters in width. For example,\n",
    "'Racine_TR-V-1677-Phedre0000.txt'[:-8]\n",
    "# alternatively, we could use a regular expression:\n",
    "import re\n",
    "re.sub(r'[0-9]+\\.txt$','', 'Racine_TR-V-1677-Phedre0000.txt')\n",
    "\n",
    "play_names_chunks = []\n",
    "for fn in filenames:\n",
    "    play_names_chunks.append(fn[:-8])\n",
    "\n",
    "num_plays = len(set(play_names_chunks))\n",
    "doctopic = np.zeros((num_plays, num_topics))\n",
    "play_row_pairs = zip(play_names_chunks, doctopic_chunks)\n",
    "play_names = []\n",
    "for i, (name, pairs) in enumerate(itertools.groupby(play_row_pairs, key=operator.itemgetter(0))):\n",
    "    rows = [row for _, row in pairs]\n",
    "    doctopic[i, :] = sum(rows) / len(rows)\n",
    "    play_names.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have used all the other plays to fit the NMF model—in deriving the\n",
    "topic components and the word-topic associations—we care principally about\n",
    "Racine’s atypical plays and the synthetic “average” play that will serve as a proxy for\n",
    "a “normal” Racine play. We will construct the average play by averaging the\n",
    "shares of the typical plays (i.e., all those that are not atypical):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "racine_plays = [name for name in play_names if name.startswith('Racine')]\n",
    "racine_atypical = ['Racine_TR-V-1664-Thebaide', 'Racine_TR-V-1677-Phedre', 'Racine_TR-V-1689-Esther', 'Racine_TR-V-1691-Athalie']\n",
    "racine_typical = [name for name in racine_plays if name not in racine_atypical]\n",
    "# alternatively, an opportunity to use set difference\n",
    "# racine_typical = list(set(racine_plays) - set(racine_atypical))\n",
    "\n",
    "# examine the list of typical plays, making sure we have the right ones\n",
    "racine_typical\n",
    "\n",
    "doctopic_racine_typical = np.mean(doctopic[np.in1d(play_names, racine_typical)], axis=0)\n",
    "doctopic_racine_atypical = doctopic[np.in1d(play_names, racine_atypical)]\n",
    "\n",
    "# stack the typical and the atypical plays by row\n",
    "doctopic_of_interest = np.row_stack([doctopic_racine_typical, doctopic_racine_atypical])\n",
    "\n",
    "# as a last and final step we need to keep track of the names\n",
    "# note that some of the manipulation of names and rows is fragile and relies on the names\n",
    "# being sorted alphabetically. If this were a concern we might use a pandas DataFrame\n",
    "# instead, as row and column names can be explicitly assigned\n",
    "play_names_of_interest = ['Racine-1666-1674-AVERAGE'] + racine_atypical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our matrix of document-topic proportions for the atypical plays\n",
    "and the composite Racine play, we can visualize the topic shares using\n",
    "a heatmap, a procedure which should be familiar from\n",
    "[Visualizing topic models](topic_model_visualization.html#topic-model-visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@suppress\n",
    "plt.figure(figsize=(11.3, 7))  # reset figure size\n",
    "\n",
    "plt.pcolor(doctopic_of_interest, norm=None, cmap='Blues')\n",
    "\n",
    "topic_labels = ['Topic #{}'.format(k) for k in range(num_topics)]\n",
    "plt.xticks(np.arange(doctopic_of_interest.shape[1]) + 0.5, topic_labels);\n",
    "plt.yticks(np.arange(doctopic_of_interest.shape[0]) + 0.5, play_names_of_interest);\n",
    "\n",
    "# flip the y-axis so the texts are in the order we anticipate\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# rotate the ticks on the x-axis\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# add a legend\n",
    "plt.colorbar(cmap='Blues')\n",
    "\n",
    "@savefig plot_racine_doctopic_heatmap.png width=8in\n",
    "plt.tight_layout()  # fixes margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this heatmap, a number of topics stand out as ones which we might\n",
    "wish to examine. In this case there is no harm in visually identifying the\n",
    "topics that vary the most (using our eyes).  However, were we\n",
    "confronted with a greater number of topics (say, 100 or 200 topics), such\n",
    "a procedure would be tedious and error prone. We may as well come up with\n",
    "a systematic way of identifying topics that vary substantially across texts of\n",
    "interest. One way of doing this would be to calculate the standard deviation of\n",
    "the document-topic shares across the topics. (Calculating the [entropy](https://en.wikipedia.org/wiki/Entropy) for topic-document associations would\n",
    "also be a useful measure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# examine topics of interest by ranking them by standard deviation\n",
    "# reminder: NumPy's standard deviation differs from R's standard deviation. If you\n",
    "# want them to return identical results include the argument ``ddof=1``.\n",
    "# Essentially,  NumPy's standard deviation divides the variance by ``n`` whereas R\n",
    "# uses ``n-1`` (which is preferable as it gives an unbiased estimate of the variance).\n",
    "# Using ``ddof=1`` makes NumPy use ``n-1``.\n",
    "topics_by_std = np.argsort(np.std(doctopic_of_interest, axis=0, ddof=1))[::-1]\n",
    "topics_by_std[0:10]\n",
    "\n",
    "# First we gather the words most associated with each topic\n",
    "num_top_words = 17\n",
    "topic_words = []\n",
    "@suppress\n",
    "assert len(clf.components_[0]) == len(vocab)\n",
    "for topic in clf.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words.append([vocab[i] for i in word_idx])\n",
    "\n",
    "# Now we examine the topic-word distributions for the topics that vary the most\n",
    "for t in topics_by_std[0:5]:\n",
    "    topic_words_str = ' '.join(str(t) for t in topic_words[t])\n",
    "    print(\"Topic {}: {}\".format(t, topic_words_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our ranking indicates, most of the mystery about the atypical plays is\n",
    "resolved by inspecting topics #5 and #9. (Only *Phèdre* (1677) needs\n",
    "additional scrutiny.) Given what we know about Racine’s biography, topic #5\n",
    "(dieu, temple, chrétiens) does not require a great deal of additional\n",
    "explanation. Topic #9 is more strongly associated with *Thébaïde* (1664) than\n",
    "with any other play. Inspecting the words associated with topic #9 we see it\n",
    "features words such as “et” and “un”. If we read the text of the play it\n",
    "appears that these words do indeed appear comparatively frequently. While we\n",
    "will leave it to Racine scholars to provide a detailed account of the\n",
    "difference, we may venture two provisional explanations: first, this was\n",
    "Racine’s first play and his style had yet to mature, and second, there is strong\n",
    "evidence that Molière contributed to the editing of the play and this fact may\n",
    "have something to do with the stylistic difference.\n",
    "\n",
    "To verify that the Topic #9 does indeed capture a salient difference, we may\n",
    "compare the rates of the words “et” and “un” across all Racine\n",
    "plays. The rate of “et” in *Thébaïde* does indeed stand out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# reassemble the document-term matrix\n",
    "data_dir = 'data/french-tragedy/'\n",
    "filenames = np.array(sorted(os.listdir(data_dir)))\n",
    "filenames_with_path = [os.path.join(data_dir, fn) for fn in filenames]\n",
    "vectorizer = text.CountVectorizer(input='filename')\n",
    "dtm = vectorizer.fit_transform(filenames_with_path)\n",
    "dtm = dtm.toarray()\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "authors = np.array([fn.split('_')[0] for fn in filenames])\n",
    "\n",
    "# convert to rates per 1000 words as this is easier to interpret\n",
    "dtm = 1000 * dtm / np.sum(dtm, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "for word in ['et', 'un']:\n",
    "    print(\"Rate per 1,000 words of {}\".format(word))\n",
    "    filenames_racine = filenames[authors == 'Racine']\n",
    "    rates_racine = dtm[authors == 'Racine', vocab == word]\n",
    "    for filename, rate in zip(filenames_racine, rates_racine):\n",
    "        # use some fancy formatting, see http://docs.python.org/3.3/library/string.html#formatspec\n",
    "        print(\"{:>40s}: {:.1f}\".format(filename, rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the Christian vocabulary associated with topic #5, *Esther* and\n",
    "*Athalie* also distinguish themselves through an *absence* of topic #10. Looking\n",
    "at the words associated with these topics a pattern emerges: the words are\n",
    "associated with narration or dialogue. Topic #10 includes the first person\n",
    "singular pronouns “je” and “me” along with the first person singular forms of\n",
    "the verbs “être” and “pouvoir” (“suis” and “puis”). Do Racine’s final plays\n",
    "perhaps feature dialogue to a lesser degree than Racine’s other plays?\n",
    "\n",
    "Again, to validate the suspicion that the words “je” and “me” do indeed appear\n",
    "more frequently in the final plays we will look directly at their word rates.\n",
    "The low rates of “je” and “me” in the final two plays certainly do stand out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "for word in ['je', 'me']:\n",
    "    print(\"Rate per 1,000 words of {}\".format(word))\n",
    "    filenames_racine = filenames[authors == 'Racine']\n",
    "    rates_racine = dtm[authors == 'Racine', vocab == word]\n",
    "    for filename, rate in zip(filenames_racine, rates_racine):\n",
    "        # use some fancy formatting, see http://docs.python.org/3.3/library/string.html#formatspec\n",
    "        print(\"{:>40s}: {:.1f}\".format(filename, rate))\n",
    "    print()  # print a blank line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we turn back to *Phèdre* (1677).  In terms of topic proportions,\n",
    "*Phèdre* (1677) looks similar to the composite “average” Racine play. Inspecting\n",
    "the dendrogram and the multidimensional scaling plot, we observe that the play\n",
    "is, in fact, not so different from Racine’s other plays; it stands out not\n",
    "because it is as atypical as those discussed above but because it happens to be\n",
    "similar (in terms of cosine distance) to several of Voltaire’s plays.\n",
    "Investigating why the works of a radical Enlightenment figure like Voltaire\n",
    "should so strongly resemble Racine’s is left as an exercise for the reader.\n",
    "\n",
    "<p><a id=fn-authors href=#fn-authors-link><strong>[1]</strong></a> This tutorial was written by [Allen Riddell](http://ariddell.org).\n",
    "and [Christof Schöch](http://www.christof-schoech.de).\n",
    "\n",
    "<p><a id=fn-authorship-attribution href=#fn-authorship-attribution-link><strong>[2]</strong></a> Such signatures do not always appear.\n",
    "They can be eliminated with some modest effort on the part of the writer\n",
    "[[BAG11]](references.html#brennan-adversarial-2011). There are also many instances of writers\n",
    "changing their style over time—Henry James is an excellent example\n",
    "[[Hoo07]](references.html#hoover-corpus-2007).)"
   ]
  }
 ],
 "metadata": {
  "date": 1577189965.3234055,
  "filename": "case_study_racine.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Case study: Racine’s early and late tragedies 1"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}