{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='index-0'></a>\n",
    "\n",
    "<a id='preprocessing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Frequently the texts we have are not those we want to analyze. We may have an\n",
    "single file containing the collected works of an author although we are only\n",
    "interested in a single work. Or we may be given a large work broken up into\n",
    "volumes (this is the case for *Les Misèrables*, as we will see later) where the\n",
    "division into volumes is not important to us.\n",
    "\n",
    "If we are interested in an author’s style, we likely want to break up a long\n",
    "text (such as a book-length work) into smaller chunks so we can get a sense of\n",
    "the variability in an author’s writing. If we are comparing one group of writers\n",
    "to a second group, we may wish to aggregate information about writers belonging\n",
    "to the same group. This will require merging documents or other information that\n",
    "were initially separate. This section illustrates these two common preprocessing\n",
    "step: splitting long texts into smaller “chunks” and aggregating texts together.\n",
    "\n",
    "Another important preprocessing step is tokenization. This is the process of\n",
    "splitting a text into individual words or sequences of words (*n-grams*).\n",
    "Decisions regarding tokenization will depend on the language(s) being studied\n",
    "and the research question. For example, should the phrase `\"her father's\n",
    "arm-chair\"` be tokenized as as `[\"her\", \"father\", \"s\", \"arm\", \"chair\"]` or\n",
    "`[\"her\", \"father's\", \"arm-chair\"]`. Tokenization patterns that work for one\n",
    "language may not be appropriate for another (What is the appropriate\n",
    "tokenization of “Qu’est-ce que c’est?”?). This section begins with a brief\n",
    "discussion of tokenization before covering splitting and merging texts.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Each tutorial is self-contained and should be read through in order.\n",
    "Variables and functions introduced in one subsection will be referenced and used\n",
    "in subsequent subsections. For example, the NumPy library `numpy` is\n",
    "imported and then used later without being imported a second time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "There are many ways to tokenize a text. Often ambiguity is inescapable.\n",
    "Consider the following lines of Charlotte Brontë’s *Villette*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "whose walls gleamed with foreign mirrors. Near the hearth\n",
    "appeared a little group: a slight form sunk in a deep arm-\n",
    "chair, one or two women busy about it, the iron-grey gentle-\n",
    "man anxiously looking on. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the appropriate tokenization include “armchair” or “arm-chair”? While it\n",
    "would be strange to see “arm-chair” in print today, the hyphenated version\n",
    "predominates in *Villette* and other texts from the same period. “gentleman”,\n",
    "however, seems preferable to “gentle-man,” although the [latter occurs in early\n",
    "nineteenth century English-language books](http://books.google.com/ngrams/graph?content=gentle-man&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=).\n",
    "This is a case where a simple tokenization rule (resolve end-of-line hyphens)\n",
    "will not cover all cases. For very large corpora containing a diversity of\n",
    "authors, idiosyncrasies resulting from tokenization tend not to be particularly\n",
    "consequential (“arm-chair” is not a high frequency word). For smaller corpora,\n",
    "however, decisions regarding tokenization can make a profound difference.\n",
    "\n",
    "Languages that do not mark word boundaries present an additional challenge.\n",
    "Chinese and [Classical Greek](http://163.1.169.40/cgi-bin/library?e=q-000-00---0POxy--00-0-0--0prompt-10---4----de0--0-1l--1-en-50---20-about-aristotle--00031-001-0-0utfZz-8-00&a=d&c=POxy&cl=search&d=HASHe8016f6b58790a2918de2b)\n",
    "provide two important examples. Consider the following sequence of Chinese\n",
    "characters: 爱国人.  This sequence could be broken up into the following tokens:\n",
    "[“爱”， 国人”] (to love one’s compatriots) or [“爱国”, “人”] (a country-loving\n",
    "person).  Resolving this kind of ambiguity (when it can be resolved) is an\n",
    "active topic of research. For Chinese and for other languages with this feature\n",
    "there are a number of tokenization strategies in circulation.\n",
    "\n",
    "Here are a number of examples of tokenizing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# note: there are three spaces between \"at\" and \"her\" to make the example more\n",
    "# realistic (texts are frequently plagued by such idiosyncracies)\n",
    "text = \"She looked at   her father's arm-chair.\"\n",
    "text_fr = \"Qu'est-ce que c'est?\"\n",
    "\n",
    "# tokenize on spaces\n",
    "text.split(' ')\n",
    "text_fr.split(' ')\n",
    "\n",
    "# scikit-learn\n",
    "# note that CountVectorizer discards \"words\" that contain only one character, such as \"s\"\n",
    "# CountVectorizer also transforms all words into lowercase\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CountVectorizer().build_tokenizer()(text)\n",
    "CountVectorizer().build_tokenizer()(text_fr)\n",
    "\n",
    "# nltk word_tokenize uses the TreebankWordTokenizer and needs to be given\n",
    "# a single sentence at a time.\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(text)\n",
    "word_tokenize(text_fr)\n",
    "\n",
    "# nltk PunktWordTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(text)\n",
    "tokenizer.tokenize(text_fr)\n",
    "\n",
    "# use of maketrans to tokenize on spaces, stripping punctuation\n",
    "# see python documentation for string.translate\n",
    "# string.punctuation is simply a list of punctuation\n",
    "import string\n",
    "table = str.maketrans({ch: None for ch in string.punctuation})\n",
    "[s.translate(table) for s in text.split(' ') if s != '']\n",
    "[s.translate(table) for s in text_fr.split(' ') if s != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Often we want to count inflected forms of a word together. This procedure is\n",
    "referred to as *stemming*. Stemming a German text treats the following words as\n",
    "instances of the word “Wald”: “Wald”, “Walde”, “Wälder”, “Wäldern”, “Waldes”,\n",
    "and “Walds”. Analogously, in English the following words would be counted as\n",
    "“forest”: “forest”, “forests”, “forested”, “forest’s”, “forests’”. As stemming\n",
    "reduces the number of unique vocabulary items that need to be tracked, it speeds\n",
    "up a variety of computational operations. For some kinds of analyses, such as\n",
    "authorship attribution or fine-grained stylistic analyses, stemming may obscure\n",
    "differences among writers. For example, one author may be distinguished by the\n",
    "use of a plural form of a word.\n",
    "\n",
    "NLTK offers stemming for a variety of languages in the [nltk.stem package](http://nltk.org/api/nltk.stem.html). The following code illustrates the use of the popular\n",
    "Snowball stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import GermanStemmer\n",
    "\n",
    "stemmer = GermanStemmer()\n",
    "\n",
    "# note that the stem function works one word at a time\n",
    "words = [\"Wald\", \"Walde\", \"Wälder\", \"Wäldern\", \"Waldes\", \"Walds\"]\n",
    "\n",
    "[stemmer.stem(w) for w in words]\n",
    "\n",
    "# note that the stemming algorithm \"understands\" grammar to some extent and that if \"Waldi\" were to appear in a text, it would not be stemmed.\n",
    "stemmer.stem(\"Waldi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Splitting a long text into smaller samples is a common task in text analysis. As\n",
    "most kinds of quantitative text analysis take as inputs an unordered list of\n",
    "words, breaking a text up into smaller chunks allows one to preserve context\n",
    "that would otherwise be discarded; observing two words together in\n",
    "a paragraph-sized chunk of text tells us much more about the relationship\n",
    "between those two words than observing two words occurring together in an\n",
    "100,000 word book. Or, as we will be using a selection of tragedies as our\n",
    "examples, we might consider the difference between knowing that two character\n",
    "names occur in the same scene versus knowing that the two names occur in the\n",
    "same play.\n",
    "\n",
    "To demonstrate how to divide a large text into smaller chunks, we will be\n",
    "working with the [corpus of French tragedies](datasets.html#datasets). The following\n",
    "shows the first plays in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# plays are in the directory data/french-tragedy\n",
    "# gather all the filenames, sorted alphabetically\n",
    "corpus_path = os.path.join('data', 'french-tragedy')\n",
    "\n",
    "# look at the first few filenames\n",
    "# (we are sorting because different operating systems may list files in different orders)\n",
    "sorted(os.listdir(path=corpus_path))[0:5]\n",
    "\n",
    "# we will need the entire path, e.g., 'data/Crebillon_TR-V-1703-Idomenee.txt'\n",
    "# rather than just 'Crebillon_TR-V-1703-Idomenee.txt' alone.\n",
    "tragedy_filenames = [os.path.join(corpus_path, fn) for fn in sorted(os.listdir(corpus_path))]\n",
    "\n",
    "@suppress\n",
    "tragedy_filenames_orig = tragedy_filenames.copy()\n",
    "\n",
    "# alternatively, using the Python standard library package 'glob'\n",
    "import glob\n",
    "tragedy_filenames = glob.glob(corpus_path + os.sep + '*.txt')\n",
    "\n",
    "@suppress\n",
    "assert sorted(tragedy_filenames) == sorted(tragedy_filenames_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every 1,000 words\n",
    "\n",
    "One way to split a text is to read through it and create a chunk every *n*\n",
    "words, where *n* is a number such as 500, 1,000 or 10,000. The following\n",
    "function accomplishes this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def split_text(filename, n_words):\n",
    "    \"\"\"Split a text into chunks approximately `n_words` words in length.\"\"\"\n",
    "    input = open(filename, 'r')\n",
    "    words = input.read().split(' ')\n",
    "    input.close()\n",
    "    # in Python the above can also be accomplished with the following lines:\n",
    "    # with open(filename, 'r') as input:\n",
    "    #     words = input.read().split(' ')\n",
    "    chunks = []\n",
    "    current_chunk_words = []\n",
    "    current_chunk_word_count = 0\n",
    "    for word in words:\n",
    "        current_chunk_words.append(word)\n",
    "        current_chunk_word_count += 1\n",
    "        if current_chunk_word_count == n_words:\n",
    "            chunks.append(' '.join(current_chunk_words))\n",
    "            # start over for the next chunk\n",
    "            current_chunk_words = []\n",
    "            current_chunk_word_count = 0\n",
    "    # add the final chunk, likely fewer than `n_words` in length\n",
    "    chunks.append(' '.join(current_chunk_words) )\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To divide up the plays, we simply apply this function to each text in the\n",
    "corpus. We do need to be careful to record the original file name and chunk\n",
    "number as we will need them later. One way to keep track of these details is to\n",
    "collect them in a list of Python [dictionaries](http://docs.python.org/dev/library/stdtypes.html#mapping-types-dict). There\n",
    "will be one dictionary for each chunk, containing the original filename,\n",
    "a number for the chunk, and the text of the chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing chunks to a directory\n",
    "\n",
    "These chunks may be saved in a directory for reference or for analysis in\n",
    "another program (such as MALLET or R)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# make sure the directory exists\n",
    "output_dir = '/tmp/'\n",
    "for chunk in chunks:\n",
    "    basename = os.path.basename(chunk['filename'])\n",
    "    fn = os.path.join(output_dir,\n",
    "                      \"{}{:04d}\".format(basename, chunk['number']))\n",
    "    with open(fn, 'w') as f:\n",
    "        f.write(chunk['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A stand-alone script for splitting texts is available:\n",
    "<a href=split-text.py download>split-text.py</a>.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every paragraph\n",
    "\n",
    "It is possible to split a document into paragraph-length chunks. Finding the\n",
    "appropriate character (sequence) that marks a paragraph boundary requires\n",
    "familiarity with how paragraphs are encoded in the text file. For example, the\n",
    "version of *Jane Eyre* provided in the [austen-brontë corpus](datasets.html#datasets),\n",
    "contains no line breaks within paragraphs inside chapters, so the paragraph\n",
    "marker in this case is simply the newline. Using the `split` string method\n",
    "with the newline as the argument (`split('\\n')`) will break the text into\n",
    "paragraphs. That is, if the text of *Jane Eyre* is contained in the variable\n",
    "`text` then the following sequence will split the document into\n",
    "paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "text = \"There was no possibility of taking a walk that day. We had been wandering, indeed, in the leafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no company, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so penetrating, that further out-door exercise was now out of the question.\\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the coming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the chidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to Eliza, John, and Georgiana Reed.\"\n",
    "text\n",
    "paragraphs = text.split('\\n')\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast, in the [Project Gutenberg edition of Brontë’s novel](http://www.gutenberg.org/cache/epub/1260/pg1260.txt), paragraphs are set off\n",
    "by two newlines in sequence. We still use the `split` method but we will use\n",
    "two newlines `\\n\\n` as our delimiter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "text = \"There was no possibility of taking a walk that day.  We had been\\nwandering, indeed, in the leafless shrubbery an hour in the morning; but\\nsince dinner (Mrs. Reed, when there was no company, dined early) the cold\\nwinter wind had brought with it clouds so sombre, and a rain so\\npenetrating, that further out-door exercise was now out of the question.\\n\\nI was glad of it: I never liked long walks, especially on chilly\\nafternoons: dreadful to me was the coming home in the raw twilight, with\\nnipped fingers and toes, and a heart saddened by the chidings of Bessie,\\nthe nurse, and humbled by the consciousness of my physical inferiority to\\nEliza, John, and Georgiana Reed.\"\n",
    "\n",
    "text\n",
    "paragraphs = text.split('\\n\\n')\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='grouping-texts'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "When comparing groups of texts, we often want to aggregate information about the\n",
    "texts that comprise each group. For instance, we may be interested in comparing\n",
    "the works of one author with the works of another author. Or we may be\n",
    "interested in comparing texts published before 1800 with texts published after\n",
    "1800. In order to do this, we need a strategy for collecting information (often\n",
    "word frequencies) associated with every text in a group.\n",
    "\n",
    "As an illustration, consider the task of grouping word frequencies in French\n",
    "tragedies by author. We have four authors (Crébillon, Corneille, Racine, and\n",
    "Voltaire) and 60 texts. Typically the first step in grouping texts together is\n",
    "determining what criterion or “key” defines a group. In this case the key is the\n",
    "author, which is conveniently recorded at the beginning of each filename in our\n",
    "corpus. So our first step will be to associate each text (the contents of each\n",
    "file) with the name of its author. As before we will use a list of dictionaries\n",
    "to manage our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# in every filename the author's last name is followed by an underscore ('_'),\n",
    "# for example: Voltaire_TR-V-1764-Olympie.txt\n",
    "\n",
    "# os.path.basename(...) gets us the filename from a path, e.g.,\n",
    "os.path.basename('french-tragedy/Voltaire_TR-V-1764-Olympie.txt')\n",
    "\n",
    "# using the split method we can break up the string on the underscore ('_')\n",
    "os.path.basename('french-tragedy/Voltaire_TR-V-1764-Olympie.txt').split('_')\n",
    "\n",
    "# putting these two steps together\n",
    "author = os.path.basename('french-tragedy/Voltaire_TR-V-1764-Olympie.txt').split('_')[0]\n",
    "author\n",
    "\n",
    "# and for all the authors\n",
    "authors = [os.path.basename(filename).split('_')[0] for filename in tragedy_filenames]\n",
    "authors\n",
    "\n",
    "# to ignore duplicates we can transform the list into a set (which only records unique elements)\n",
    "set(authors)\n",
    "\n",
    "# as there is no guarantee about the ordering in a set (or a dictionary) we will typically\n",
    "# first drop duplicates and then save our unique names as a sorted list. Because there are\n",
    "# no duplicates in this list, we can be confident that the ordering is the same every time.\n",
    "sorted(set(authors))\n",
    "\n",
    "# and we have a way of finding which indexes in authors correspond to each author using array indexing\n",
    "authors = np.array(authors)  # convert from a Python list to a NumPy array\n",
    "first_author = sorted(set(authors))[0]\n",
    "first_author\n",
    "authors == first_author\n",
    "np.nonzero(authors == first_author)  # if we want the actual indexes\n",
    "authors[np.nonzero(authors == first_author)]\n",
    "\n",
    "# alternatively, we can find those indexes of texts *not* written by `first_author`\n",
    "authors[authors != first_author]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to group the data is to use NumPy’s array indexing. This method\n",
    "is more concise than the alternatives and it should be familiar to those\n",
    "comfortable with R or Octave/Matlab. (Those for whom this method is unfamiliar\n",
    "will benefit from reviewing the introductions to NumPy mentioned in\n",
    "[Getting started](getting_started.html#getting-started).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "authors = np.array([os.path.basename(filename).split('_')[0] for filename in tragedy_filenames])\n",
    "\n",
    "# allocate an empty array to store our aggregated word frequencies\n",
    "authors_unique = sorted(set(authors))\n",
    "dtm_authors = np.zeros((len(authors_unique), len(vocab)))\n",
    "for i, author in enumerate(authors_unique):\n",
    "    dtm_authors[i, :] = np.sum(dtm[authors==author, :], axis=0)\n",
    "\n",
    "@suppress\n",
    "dtm_authors_method_numpy = dtm_authors.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**\n",
    ">\n",
    ">Recall that gathering together the sum of the entries along columns is\n",
    "performed with `np.sum(X, axis=0)` or `X.sum(axis=0)`. This is\n",
    "the NumPy equivalent of R’s `apply(X, 2, sum)` (or `colSums(X)`).\n",
    "\n",
    "Grouping data together in this manner is such a common problem in data analysis\n",
    "that there are packages devoted to making the work easier. For example, if you\n",
    "have the [pandas library](http://pandas.pydata.org) installed, you can\n",
    "accomplish what we just did in two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "authors = [os.path.basename(filename).split('_')[0] for filename in tragedy_filenames]\n",
    "dtm_authors = pandas.DataFrame(dtm).groupby(authors).sum().values\n",
    "\n",
    "@suppress\n",
    "dtm_authors_method_pandas = dtm_authors.copy()\n",
    "\n",
    "@suppress\n",
    "np.testing.assert_array_almost_equal(dtm_authors_method_pandas, dtm_authors_method_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more general strategy for grouping data together makes use of the `groupby`\n",
    "function in the Python standard library [itertools](http://docs.python.org/dev/library/itertools.html). This method has the\n",
    "advantage of being fast and memory efficient. As a warm-up exercise, we will\n",
    "group just the filenames by author using `groupby` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "\n",
    "texts = []\n",
    "for filename in tragedy_filenames:\n",
    "    author = os.path.basename(filename).split('_')[0]\n",
    "    # the following are equivalent\n",
    "    # {'filename': filename, 'author':author}\n",
    "    # dict(filename=filename, author=author)\n",
    "    # (I find the second easier to type)\n",
    "    texts.append(dict(filename=filename, author=author))\n",
    "\n",
    "# groupby requires that the list be sorted by the 'key' with which we will be doing the grouping\n",
    "texts = sorted(texts, key=operator.itemgetter('author'))\n",
    "\n",
    "# if d is a dictionary, operator.itemgetter(key)(d) does d[key]\n",
    "d = {'number': 5}\n",
    "d['number']\n",
    "operator.itemgetter('number')(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "grouped_data = {}\n",
    "for author, grouped in itertools.groupby(texts, key=operator.itemgetter('author')):\n",
    "    grouped_data[author] = ','.join(os.path.basename(t['filename']) for t in grouped)\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding lines of code demonstrate how to group filenames by author. Now we\n",
    "want to aggregate document-term frequencies by author. The process is similar.\n",
    "We use the same strategy of creating a collection of dictionaries with the\n",
    "information we want to aggregate and the key—the author’s name—that\n",
    "identifies each group.\n",
    "\n",
    "Now that we have done the work of grouping these texts together, we can examine\n",
    "the relationships among the four authors using the exploratory techniques we\n",
    "learned in [Working with text](working_with_text.html#working-with-text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dist = 1 - cosine_similarity(dtm_authors)\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\")\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "names = sorted(set(authors))\n",
    "for x, y, name in zip(xs, ys, names):\n",
    "    color = matplotlib.cm.summer(names.index(name))\n",
    "    plt.scatter(x, y, c=color)\n",
    "    plt.text(x, y, name)\n",
    "\n",
    "@savefig plot_preprocessing_authors_mds.png width=7in\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is possible to group texts by any feature they share in common.\n",
    "If, for instance, we had wanted to organize our texts into 50 year periods\n",
    "(1650-1699, 1700-1749, …) rather than by author, we would begin by extracting\n",
    "the publication year from the filename.\n",
    "\n",
    "Then we would create a list of group identifiers based on the periods that\n",
    "interest us:\n",
    "\n",
    "Finally we would group the texts together using the same procedure as we did\n",
    "with authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "periods_unique = sorted(set(periods))\n",
    "dtm_periods = np.zeros((len(periods_unique), len(vocab)))\n",
    "for i, period in enumerate(periods_unique):\n",
    "    dtm_periods[i,:] = np.sum(dtm[periods==period,:], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Write a tokenizer that, as it tokenizes, also transforms uppercase words into\n",
    "  lowercase words. Consider using the string method `lower`.  \n",
    "1. Using your tokenizer, count the number of times `green` occurs in the\n",
    "  following text sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "\"I find,\" Mr. Green said, \"that there are many members here who do not know\n",
    "me yet,--young members, probably, who are green from the waste lands and\n",
    "road-sides of private life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Personal names that occur in lowercase form in the dictionary illustrate one\n",
    "  kind of information that is lost by ignoring case. Provide another example of\n",
    "  useful information lost when lowercasing all words.  "
   ]
  }
 ],
 "metadata": {
  "date": 1577189965.8721519,
  "filename": "preprocessing.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Preprocessing"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}