{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='index-0'></a>\n",
    "\n",
    "<a id='feature-selection'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection: finding distinctive words\n",
    "\n",
    "We often want to know what words distinguish one group of texts from another\n",
    "group of texts. For instance, we might be working with an archive of two city\n",
    "newspapers, say, the *Frankfurter Allgemeine Zeitung* and the *Frankfurter\n",
    "Rundschau* and want to know which words tend to appear in one newspaper rather\n",
    "than the other. Or we might be interested in comparing word usage in US\n",
    "Presidents’ [State of the Union addresses](http://en.wikipedia.org/wiki/State_of_the_Union_address) given during\n",
    "recessions with addresses given during periods of economic growth. Or we might\n",
    "be comparing the style of several novelists and want to know if one author tends\n",
    "to use words not found in the works of others.\n",
    "\n",
    "This section illustrates how distinctive words can be identified using a corpus\n",
    "of novels containing works by two authors: Jane Austen and Charlotte Brontë.\n",
    "\n",
    "- Austen, *Emma*  \n",
    "- Austen, *Pride and Prejudice*  \n",
    "- Austen, *Sense and Sensibility*  \n",
    "- C. Brontë, *Jane Eyre*  \n",
    "- C. Brontë, *The Professor*  \n",
    "- C. Brontë, *Villette*  \n",
    "\n",
    "\n",
    "This [corpus of six novels](datasets.html#datasets) consists of the following text\n",
    "files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filenames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0502efd65a76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'filenames' is not defined"
     ]
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will find that among the words that reliably distinguish Austen from Brontë\n",
    "are  “such”, “could”, and “any”. This tutorial demonstrates how we arrived at\n",
    "these words.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">The following features an introduction to the concepts underlying\n",
    "feature selection. Those who are working with a very large corpus and are\n",
    "familiar with statistics may wish to skip ahead to the section on\n",
    "[group comparison](#bayesian-group-comparison) or the section\n",
    "[Log likelihood ratio and \\chi^2 feature selection](#chi2).\n",
    "\n",
    "Since we are concerned with words, we begin by extracting word frequencies from\n",
    "each of the texts in our corpus and [construct a document-term matrix](working_with_text.html#working-with-text) that records the rate per 1,000 words for each word\n",
    "appearing in the corpus.  Using rates rather than counts will allow us to\n",
    "account for differences in the length of the novels. Accounting for differences\n",
    "in document lengths when dealing with word counts is essential. For example,\n",
    "a text using “whence” ten times in a 1,000 word article uses the word at a rate\n",
    "of 10 per 1,000 words, while a 100,000 word novel that uses “whence” 20 times\n",
    "uses it at a rate of 0.2 per 1,000 words. While the word occurs more in absolute\n",
    "terms in the second text, the rate is higher in the first text. While there are\n",
    "other ways to account for document length—a procedure called\n",
    "“normalization”—considering the rate per 1,000 words will serve us well. An\n",
    "appealing feature of word rates per 1,000 words is that readers are familiar\n",
    "with documents of this length (e.g., a newspaper article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a6e5f6eab407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# just examine a sample, those at offsets 100 to 105\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m105\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m105\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rates' is not defined"
     ]
    }
   ],
   "source": [
    "# just examine a sample, those at offsets 100 to 105\n",
    "rates[:, 100:105]\n",
    "vocab[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring “distinctiveness”\n",
    "\n",
    "Finding distinctive words requires a decision about what “distinctive” means.\n",
    "As we will see, there are a variety of definitions that we might use.  It seems\n",
    "reasonable to expect that all definitions of distinctive would identify as\n",
    "distinctive words found exclusively in texts associated with a single author (or\n",
    "group). For example, if Brontë uses the word “access” and Austen never\n",
    "does, we should count “access” as distinctive. A more challenging question is\n",
    "how to treat words that occur in both groups of texts but do so with different\n",
    "rates.\n",
    "\n",
    "Finding words that are unique to a group is a simple exercise. Indeed, it is\n",
    "worth treating these words a special case so they will not clutter our work\n",
    "later on. We will quickly identify these words and remove them. (They tend not\n",
    "to be terribly interesting words.)\n",
    "\n",
    "A simple way of identifying words unique to one author would be to calculate the\n",
    "average rate of word use across all texts for each author and then to look for\n",
    "cases where the average rate is zero for one author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filenames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6c6a1209a632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# indices so we can refer to the rows for the relevant author\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mausten_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbronte_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"Austen\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mausten_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filenames' is not defined"
     ]
    }
   ],
   "source": [
    "# indices so we can refer to the rows for the relevant author\n",
    "austen_indices, cbronte_indices = [], []\n",
    "for index, fn in enumerate(filenames):\n",
    "    if \"Austen\" in fn:\n",
    "        austen_indices.append(index)\n",
    "    elif \"CBronte\" in fn:\n",
    "        cbronte_indices.append(index)\n",
    "\n",
    "# this kind of slicing should be familiar if you've used R or Octave/Matlab\n",
    "austen_rates = rates[austen_indices, :]\n",
    "cbronte_rates = rates[cbronte_indices, :]\n",
    "\n",
    "# np.mean(..., axis=0) calculates the column-wise mean\n",
    "austen_rates_avg = np.mean(austen_rates, axis=0)\n",
    "cbronte_rates_avg = np.mean(cbronte_rates, axis=0)\n",
    "\n",
    "# since zero times any number is zero, this will identify documents where\n",
    "# any author's average rate is zero\n",
    "distinctive_indices = (austen_rates_avg * cbronte_rates_avg) == 0\n",
    "\n",
    "# examine words that are unique, ranking by rates\n",
    "np.count_nonzero(distinctive_indices)\n",
    "ranking = np.argsort(austen_rates_avg[distinctive_indices] + cbronte_rates_avg[distinctive_indices])[::-1]  # from highest to lowest; [::-1] reverses order.\n",
    "vocab[distinctive_indices][ranking]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified these words, we will remove them from our corpus in\n",
    "order to focus on identifying distinctive words that appear in texts associated\n",
    "with every author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-42f2ee05c1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistinctive_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistinctive_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistinctive_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# recalculate variables that depend on rates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtm' is not defined"
     ]
    }
   ],
   "source": [
    "dtm = dtm[:, np.invert(distinctive_indices)]\n",
    "rates = rates[:, np.invert(distinctive_indices)]\n",
    "vocab = vocab[np.invert(distinctive_indices)]\n",
    "\n",
    "# recalculate variables that depend on rates\n",
    "austen_rates = rates[austen_indices, :]\n",
    "cbronte_rates = rates[cbronte_indices, :]\n",
    "austen_rates_avg = np.mean(austen_rates, axis=0)\n",
    "cbronte_rates_avg = np.mean(cbronte_rates, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in averages\n",
    "\n",
    "How can we identify a distinctive word? One approach would compare the average\n",
    "rate at which authors use a word. A simple quantitative comparison would\n",
    "calculate the difference between the rates. If one author uses a word often\n",
    "across his or her oeuvre and another barely uses the word at all, then we\n",
    "suspect the difference in rates will be large.  This will be the first\n",
    "definition of distinctiveness (sometimes called “keyness”) we will consider.\n",
    "Using this measure we can calculate the top ten distinctive words in the\n",
    "Austen-Brontë comparison as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'austen_rates_avg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b8141cf73991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# calculate absolute value because we only care about the magnitude of the difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkeyness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mausten_rates_avg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcbronte_rates_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# from highest to lowest; [::-1] reverses order in Python sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'austen_rates_avg' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# calculate absolute value because we only care about the magnitude of the difference\n",
    "keyness = np.abs(austen_rates_avg - cbronte_rates_avg)\n",
    "ranking = np.argsort(keyness)[::-1]  # from highest to lowest; [::-1] reverses order in Python sequences\n",
    "\n",
    "# print the top 10 words along with their rates and the difference\n",
    "vocab[ranking][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a start. The problem with this measure is that it tends to highlight\n",
    "differences in very frequent words. For example, this method\n",
    "gives greater attention to a word that occurs\n",
    "30 times per 1,000 words in Austen and 25 times per 1,000 in Brontë\n",
    "than it does to a word that occurs 5 times per 1,000 words in\n",
    "Austen and 0.1 times per 1,000 words in Brontë. This does not seem\n",
    "right. It seems important to recognize cases when one author uses a word\n",
    "frequently and another author barely uses it.\n",
    "\n",
    "As this initial attempt suggests, identifying distinctive words will be\n",
    "a balancing act. When comparing two groups of texts differences in the rates of\n",
    "frequent words will tend to be large relative to differences in the rates of\n",
    "rarer words. Human language is variable; some words occur more frequently than\n",
    "others regardless of who is writing.  We need to find a way of adjusting our\n",
    "definition of distinctive in light of this.\n",
    "\n",
    "One adjustment that is easy to make is to divide the difference in authors’\n",
    "average rates by the average rate across all authors. Since dividing a quantity\n",
    "by a large number will make that quantity smaller, our new distinctiveness score\n",
    "will tend to be lower for words that occur frequently. While this is merely\n",
    "a heuristic, it does move us in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f130ced33065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# cbronte_rates_avg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrates_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mkeyness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mausten_rates_avg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcbronte_rates_avg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrates_avg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rates' is not defined"
     ]
    }
   ],
   "source": [
    "# we have already calculated the following quantities\n",
    "# austen_rates_avg\n",
    "# cbronte_rates_avg\n",
    "\n",
    "rates_avg = np.mean(rates, axis=0)\n",
    "\n",
    "keyness = np.abs(austen_rates_avg - cbronte_rates_avg) / rates_avg\n",
    "ranking = np.argsort(keyness)[::-1]  # from highest to lowest; [::-1] reverses order.\n",
    "\n",
    "# print the top 10 words along with their rates and the difference\n",
    "vocab[ranking][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method improves on our initial attempt. It has\n",
    "the virtue of being simple and easy to implement. Yet it has its flaws. For\n",
    "example, the method tends to overemphasize very rare words.\n",
    "\n",
    "Just as there are many definitions of “similarity” or “distance” available to\n",
    "compare two texts (see [Working with text](working_with_text.html#working-with-text)), there are many definitions of\n",
    "distinctive that can be used to identify words that characterize a group of\n",
    "texts.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">While we used the absolute value of the difference in average rates,\n",
    "$ |x-y| $ we might have easily used the squared difference,\n",
    "$ (x-y)^2 $ as it has similar properties (always positive, increasing as\n",
    "difference increases).\n",
    "\n",
    "\n",
    "<a id='bayesian-group-comparison'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian group comparison\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">The following sections assume some familiarity with statistics and\n",
    "probability. Introductory texts include [[CB01]](references.html#casella-statistical-2001),\n",
    "[[Hof09]](references.html#hoff-first-2009), and [[Lee04]](references.html#lee-bayesian-2004).\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">The following excursion into the world of Bayesian inference and Gibbs\n",
    "sampling is closely related to topic modeling and Latent Dirichlet Allocation\n",
    "(LDA). The inference for the model discussed below proceeds using a Gibbs\n",
    "sampler from the full condition distribution of each variable of\n",
    "interest—precisely the same procedure is used in LDA.\n",
    "\n",
    "A more nuanced comparison of word use in two groups takes account of the\n",
    "variability in word use. Consider for instance the word “green”\n",
    "in Austen and Brontë.  In Austen the word occurs with the following rates: 0.01,\n",
    "0.03, and 0.06 (0.03 on average).  In Brontë the word is consistently more\n",
    "frequent: 0.16, 0.36, and 0.22 (0.24 on average). These two groups of rates\n",
    "look different. But consider how our judgment might change if the rates observed\n",
    "in Brontë’s novels were much more variable, say, 0.03, 0.04, and 0.66 (0.24 on\n",
    "average).  Although the averages remain the same, the difference does not seem\n",
    "so pronounced; with only one observation (0.66) noticeably greater than we find in Austen, we\n",
    "might reasonably doubt that there is evidence of a systematic difference between\n",
    "the authors. <sup><a href=#fn-lyon id=fn-lyon-link>[1]</a></sup>\n",
    "\n",
    "One way of formalizing a comparison of two groups that takes account of the\n",
    "variability of word usage comes from Bayesian statistics. To describe our\n",
    "beliefs about the word frequencies we observe, we use a probability\n",
    "distribution, which we will call our a sampling model. Under the model we will\n",
    "use, the rates are assumed to come from two different normal distributions. The\n",
    "question we will be asking is how confident we are that the means of the two\n",
    "normal distributions are different. How confident we are (expressed as\n",
    "a probability) that the means are indeed different will stand in as our measure\n",
    "of distinctiveness.\n",
    "\n",
    "We will use the parameterization below for our two normal sampling\n",
    "distributions. Group 1 corresponds to Austen and group 2 corresponds to Brontë:\n",
    "\n",
    "$$\n",
    "Y_{i,1} = \\mu + \\delta + \\epsilon_{i,1}\n",
    "\n",
    "Y_{i,2} = \\mu - \\delta + \\epsilon_{i,2}\n",
    "\n",
    "\\{\\epsilon_{i,j}\\} \\sim \\textrm{i.i.d.} \\; \\textrm{Normal}(0, \\sigma^2)\n",
    "\n",
    "n = 1, 2, 3\n",
    "$$\n",
    "\n",
    "(i.i.d. stands for [independently and identically distributed](http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables))\n",
    "\n",
    "It is easy to relate this parameterization back to two normal distributions.\n",
    "Austen’s texts come from a normal distribution with mean parameter\n",
    "$ \\theta_1 = \\mu + \\delta $ and variance $ \\sigma^2 $, whereas Brontë’s\n",
    "novels come from a distribution with the same variance and with mean parameter\n",
    "$ \\theta_2 = \\mu - \\delta $. $ \\delta $ corresponds to half the\n",
    "difference between the two means and it is through this parameter that we will\n",
    "judge how confident we are of a difference between the two distributions.\n",
    "\n",
    "As we consider the question of what prior distributions to assign to\n",
    "$ \\mu $, $ \\delta $, and $ \\sigma^2 $ we need to keep in mind that\n",
    "the word rates must be positive even though we are using normal distributions\n",
    "(which will always assign some, potentially quite small, probability to negative\n",
    "values).  A compromise that will allow us to make use of\n",
    "computationally-convenient conjugate prior distributions will be to use normal\n",
    "prior distributions that favor positive values in most cases. As we will be\n",
    "modeling more than ten thousand of vocabulary items, computational speed will be\n",
    "important. These are the prior distributions that we will use:\n",
    "\n",
    "$$\n",
    "\\mu \\sim \\textrm{Normal}(\\mu_0, \\tau_0^2)\n",
    "\n",
    "\\delta \\sim \\textrm{Normal}(0, \\gamma_0^2)\n",
    "\n",
    "\\sigma^2 \\sim \\textrm{Inverse-Gamma}(\\nu_0/2, \\nu_0\\sigma_0^2/2)\n",
    "$$\n",
    "\n",
    "We need to determine suitable values for the priors’ parameters\n",
    "(called hyperparameters): $ \\mu_0,\n",
    "\\tau_0^2, \\gamma_0^2, \\nu_0 $, and $ \\sigma_0^2 $. Let us consider\n",
    "$ \\mu_0 $ and $ \\sigma_0^2 $ first. While words like “the” and “she”\n",
    "occur quite frequently, almost all words (>99%) occur less than four times per\n",
    "1,000 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9fd99f59ae06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrates\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrates\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmquantiles\u001b[0m  \u001b[0;31m# analgous to R's quantiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rates' is not defined"
     ]
    }
   ],
   "source": [
    "np.mean(rates < 4)\n",
    "\n",
    "np.mean(rates > 1)\n",
    "\n",
    "from scipy.stats.mstats import mquantiles  # analgous to R's quantiles\n",
    "mquantiles(rates, prob=[0.01, 0.5, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In keeping with this observation we will set $ \\mu_0 $ to be 3 and\n",
    "$ \\tau^2 $ to be $ 1.5^2 $, with the reasoning that when drawing\n",
    "from a normal distribution, the great majority (.95) of observations will fall\n",
    "between two standard deviations of the mean. There isn’t tremendous variability\n",
    "in rates across the works of a single author, so we will set $ \\sigma_0^2 $\n",
    "to be 1 and $ \\nu_0 $ to be 1. (If we were to use non-conjugate priors we\n",
    "could model our prior beliefs about rates more realistically.) We know there is\n",
    "considerable variability in the rates *between* authors, so we will choose\n",
    "$ \\gamma_0^2 $ to be $ 1.5^2 $, as $ \\delta $ represents half the\n",
    "difference between the means and its value is unlikely to be greater than 3 in\n",
    "absolute value.\n",
    "\n",
    "With these conjugate priors it is possible to use a Gibbs sampler to sample\n",
    "efficiently from the posterior distribution, using the full conditional\n",
    "distributions for the parameters of interest [[Hof09]](references.html#hoff-first-2009):\n",
    "\n",
    "$$\n",
    "\\{\\mu|\\mathbf{y_1}, \\mathbf{y_2}, \\delta, \\sigma^2\\} &\\sim \\textrm{Normal}(\\mu_n, \\gamma_n^2)\\\\\n",
    "    \\mu_n &= \\gamma_n^2 \\times [\\mu_0/\\gamma_0^2 + \\sum_{i=1}^{n_1} (y_{i,1} - \\delta)/\\sigma^2 +\n",
    "        \\sum_{i=1}^{n_2} (y_{i,2} - \\delta)/\\sigma^2 ] \\\\\n",
    "    \\gamma_n^2 &= [1/\\gamma_0^2 + (n_1+n_2)/\\sigma^2]^{-1} \\\\\n",
    "\n",
    "\\{\\delta|\\mathbf{y_1}, \\mathbf{y_2}, \\mu, \\sigma^2\\} &\\sim \\textrm{Normal}(\\delta_n, \\tau_n^2)\\\\\n",
    "    \\delta_n &= \\tau_n^2 \\times [ \\delta_0/\\tau_0^2 +\n",
    "        \\sum_{i=1}^{n_1} (y_{i,1} - \\mu)/\\sigma^2 - \\sum_{i=1}^{n_2} (y_{i,2} - \\mu)/\\sigma^2 ]\\\\\n",
    "    \\tau_n^2 &= [1/\\tau_0^2 + (n_1+n_2)/\\sigma^2]^{-1} \\\\\n",
    "\n",
    "\\{\\sigma^2|\\mathbf{y_1}, \\mathbf{y_2}, \\delta, \\mu\\} &\\sim \\textrm{Inverse-Gamma}(\\nu_n/2, \\nu_n\\sigma_n^2/2)\\\\\n",
    "    \\nu_n &= \\nu_0 + n_1 + n_2 \\\\\n",
    "    \\nu_n\\sigma_n^2 &= \\nu_0\\sigma_0^2 +\n",
    "        \\sum_{i=1}^{n_1} (y_{i,1} - (\\mu+\\delta)) + \\sum_{i=1}^{n_2} (y_{i,2} - (\\mu - \\delta)) \\\\\n",
    "$$\n",
    "\n",
    "In Python, we can wrap the Gibbs sampler in single function and use it to get\n",
    "a distribution of posterior values for $ \\delta $, which is the variable we\n",
    "care about in this context as it characterizes our belief about the difference\n",
    "in authors’ word usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def sample_posterior(y1, y2, mu0, sigma20, nu0, delta0, gamma20, tau20, S):\n",
    "    \"\"\"Draw samples from posterior distribution using Gibbs sampling\n",
    "    Parameters\n",
    "    ----------\n",
    "    `S` is the number of samples\n",
    "    Returns\n",
    "    -------\n",
    "    chains : dict of array\n",
    "        Dictionary has keys: 'mu', 'delta', and 'sigma2'.\n",
    "    \"\"\"\n",
    "    n1, n2 = len(y1), len(y2)\n",
    "    # initial values\n",
    "    mu = (np.mean(y1) + np.mean(y2))/2\n",
    "    delta = (np.mean(y1) - np.mean(y2))/2\n",
    "    vars = ['mu', 'delta', 'sigma2']\n",
    "    chains = {key: np.empty(S) for key in vars}\n",
    "    for s in range(S):\n",
    "        # update sigma2\n",
    "        a = (nu0+n1+n2)/2\n",
    "        b = (nu0*sigma20 + np.sum((y1-mu-delta)**2) + np.sum((y2-mu+delta)**2))/2\n",
    "        sigma2 = 1 / np.random.gamma(a, 1/b)\n",
    "        # update mu\n",
    "        mu_var = 1/(1/gamma20 + (n1+n2)/sigma2)\n",
    "        mu_mean = mu_var * (mu0/gamma20 + np.sum(y1-delta)/sigma2 +\n",
    "                            np.sum(y2+delta)/sigma2)\n",
    "        mu = np.random.normal(mu_mean, np.sqrt(mu_var))\n",
    "        # update delta\n",
    "        delta_var = 1/(1/tau20 + (n1+n2)/sigma2)\n",
    "        delta_mean = delta_var * (delta0/tau20 + np.sum(y1-mu)/sigma2 -\n",
    "                                np.sum(y2-mu)/sigma2)\n",
    "        delta = np.random.normal(delta_mean, np.sqrt(delta_var))\n",
    "        # save values\n",
    "        chains['mu'][s] = mu\n",
    "        chains['delta'][s] = delta\n",
    "        chains['sigma2'][s] = sigma2\n",
    "    return chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'austen_rates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b79b4b921e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"green\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mausten_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbronte_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# prior parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'austen_rates' is not defined"
     ]
    }
   ],
   "source": [
    "# data\n",
    "word = \"green\"\n",
    "y1, y2 = austen_rates[:, vocab == word], cbronte_rates[:, vocab == word]\n",
    "\n",
    "# prior parameters\n",
    "mu0 = 3\n",
    "tau20 = 1.5**2\n",
    "\n",
    "nu0 = 1\n",
    "sigma20 = 1\n",
    "\n",
    "delta0 = 0\n",
    "gamma20 = 1.5**2\n",
    "\n",
    "# number of samples\n",
    "S = 2000\n",
    "\n",
    "chains = sample_posterior(y1, y2, mu0, sigma20, nu0, delta0, gamma20, tau20, S)\n",
    "\n",
    "delta = chains['delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These samples reflect what our belief about $ \\delta $ ought to be given our\n",
    "prior specification. Our interest is in $ \\delta $, which represents the\n",
    "half the difference between the population means for the distributions\n",
    "characterizing word rates in Austen and Brontë. We aren’t concerned with whether\n",
    "or not it is negative or positive, but we do care whether or not it is likely to\n",
    "be zero. In fact, we need to have a measure of how confident we are that\n",
    "$ \\delta $ is something other than zero (implying no difference in means).\n",
    "If, for instance, the moment that samples of $ \\delta $ tend to be negative;\n",
    "we need to know the posterior probability of its being definitively less than\n",
    "zero, $ \\textrm{p}(\\delta < 0) $. This probability can be estimated from the\n",
    "output of the Gibbs sampler. The following demonstrates the calculation of this\n",
    "probability for two different words, ‘green’ and ‘dark’, both words more\n",
    "characteristic of the Brontë novels than the Austen novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'austen_rates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-be666db6e04d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mausten_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'green'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbronte_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'green'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_posterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnu0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdelta_green\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchains\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'austen_rates' is not defined"
     ]
    }
   ],
   "source": [
    "y1 = austen_rates[:, vocab == 'green']\n",
    "y2 = cbronte_rates[:, vocab == 'green']\n",
    "chains = sample_posterior(y1, y2, mu0, sigma20, nu0, delta0, gamma20, tau20, S)\n",
    "delta_green = chains['delta']\n",
    "\n",
    "y1 = austen_rates[:, vocab == 'dark']\n",
    "y2 = cbronte_rates[:, vocab == 'dark']\n",
    "chains = sample_posterior(y1, y2, mu0, sigma20, nu0, delta0, gamma20, tau20, S)\n",
    "delta_dark = chains['delta']\n",
    "\n",
    "# estimate of p(delta < 0)\n",
    "np.mean(delta_dark < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-ed77bf5800e8>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-ed77bf5800e8>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    assert all(vocab[ix] == words)  # order matters for subsequent display\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "words = ['dark', 'green']\n",
    "ix = np.in1d(vocab, words)\n",
    "\n",
    "@suppress\n",
    "assert all(vocab[ix] == words)  # order matters for subsequent display\n",
    "\n",
    "keyness = np.asarray([np.mean(delta_dark < 0), np.mean(delta_green < 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ‘dark’ is more distinctive of Brontë than ‘green’ is, the probabilities\n",
    "(our measure of distinctiveness or keyness) reflect this.\n",
    "\n",
    "If we want to apply this “feature selection” method *en masse* to every word\n",
    "occurring in the corpus, we need only write one short loop and make an\n",
    "adjustment for the fact that we don’t care whether or not $ \\delta $ is\n",
    "positive or negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# fewer samples to speed things up, this may take several minutes to run\n",
    "S = 200\n",
    "\n",
    "def delta_confidence(rates_one_word):\n",
    "    austen_rates = rates_one_word[0:3]\n",
    "    bronte_rates = rates_one_word[3:6]\n",
    "    chains = sample_posterior(austen_rates, bronte_rates, mu0, sigma20, nu0,\n",
    "                              delta0, gamma20, tau20, S)\n",
    "    delta = chains['delta']\n",
    "    return np.max([np.mean(delta < 0), np.mean(delta > 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide-output": false
   },
   "source": [
    "```python\n",
    "# apply the function over all columns\n",
    "In [117]: keyness = np.apply_along_axis(delta_confidence, axis=0, arr=rates)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keyness' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-940773bb84d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# from highest to lowest; [::-1] reverses order.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print the top 10 words along with their rates and the difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mranking\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keyness' is not defined"
     ]
    }
   ],
   "source": [
    "ranking = np.argsort(keyness)[::-1]  # from highest to lowest; [::-1] reverses order.\n",
    "\n",
    "# print the top 10 words along with their rates and the difference\n",
    "vocab[ranking][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a useful ordering of characteristic words. Unlikely [frequentist](https://en.wikipedia.org/wiki/Frequentist_inference) methods discussed below\n",
    "(chi-squared and log likelihood) this approach considers the variability of\n",
    "observations within groups. This method will also work for small corpora\n",
    "provided useful prior information is available. To the extent that we are\n",
    "interested in a close reading of differences of vocabulary use, the Bayesian\n",
    "method should be preferred. <sup><a href=#fn-underwood id=fn-underwood-link>[2]</a></sup>\n",
    "\n",
    "\n",
    "<a id='chi2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log likelihood ratio and $ \\chi^2 $ feature selection\n",
    "\n",
    "We can recast our discussions about measuring distinctiveness in terms of\n",
    "hypothesis testing. This turns out to be a satisfying way of thinking about the\n",
    "problem and it also allows us to introduce a range of feature selection methods,\n",
    "including the log likelihood test and the $ \\chi^2 $ test.\n",
    "\n",
    "One hypothesis that we might test comes as no surprise: rather than two groups\n",
    "of texts characterized by different word rates, this hypothesis claims that\n",
    "there is, in fact, a single group. Words are examined one at a time; those words\n",
    "for which this hypothesis seems most wrong will be counted as distinctive\n",
    "(classical statistics is always a workout in counterfactual language).\n",
    "\n",
    "Consider again the word “green”. Taking all the Austen texts together, the word\n",
    "“green” occurs 11 times out of ~370,000 words (0.03 per 1,000 words). In the\n",
    "novels by Brontë, “green” occurs 96 times out of ~400,000 (0.24 per 1,000\n",
    "words). We do not really need statistics to tell us that this is a large\n",
    "difference: picking a word from each author-specific corpus at random, one is ten\n",
    "times more likely to find “green” in the Brontë corpus. To summarize the\n",
    "appearance of the word “green” we may assemble a table with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-08472b4e3673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgreen_austen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mausten_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"green\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnongreen_austen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mausten_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgreen_austen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgreen_cbronte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcbronte_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"green\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnongreen_cbronte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcbronte_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgreen_cbronte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtm' is not defined"
     ]
    }
   ],
   "source": [
    "green_austen = np.sum(dtm[austen_indices, vocab == \"green\"])\n",
    "nongreen_austen = np.sum(dtm[austen_indices, :]) - green_austen\n",
    "green_cbronte = np.sum(dtm[cbronte_indices, vocab == \"green\"])\n",
    "nongreen_cbronte = np.sum(dtm[cbronte_indices, :]) - green_cbronte\n",
    "\n",
    "green_table = np.array([[green_austen, nongreen_austen],\n",
    "                        [green_cbronte, nongreen_cbronte]])\n",
    "green_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypothesis being tested is that the grouping of the counts by author is\n",
    "unnecessary, that $ P(word = \"green\" | author = \"Austen\") = P(word\n",
    "= \"green\" | author != \"Austen\") $. If this were the case, then the rate of\n",
    "“green” in the corpus is the same, namely 0.14 per 1,000 words, and we would\n",
    "anticipate seeing the following frequencies given the total number of words\n",
    "for each group of texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9275a0c0ab6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprob_green\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"green\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprob_notgreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprob_green\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Austen\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"Austen\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"CBrontë\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtm' is not defined"
     ]
    }
   ],
   "source": [
    "prob_green = np.sum(dtm[:, vocab == \"green\"]) / np.sum(dtm)\n",
    "prob_notgreen = 1 - prob_green\n",
    "labels = []\n",
    "for fn in filenames:\n",
    "    label = \"Austen\" if \"Austen\" in fn else \"CBrontë\"\n",
    "    labels.append(label)\n",
    "n_austen = np.sum(dtm[labels == \"Austen\", :])\n",
    "n_cbronte = np.sum(dtm[labels != \"Austen\", :])\n",
    "\n",
    "expected_table = np.array([[prob_green * n_austen, prob_notgreen * n_austen],\n",
    "                           [prob_green * n_cbronte, prob_notgreen * nongreen_cbronte]])\n",
    "expected_table\n",
    "\n",
    "# same result, but more concise and more general\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "X = dtm[:, vocab == \"green\"]\n",
    "X = np.append(X, np.sum(dtm[:, vocab != \"green\"], axis=1, keepdims=True), axis=1)\n",
    "y = LabelBinarizer().fit_transform(labels)\n",
    "y = np.append(1 - y, y, axis=1)\n",
    "green_table = np.dot(y.T, X)\n",
    "green_table\n",
    "\n",
    "feature_count = np.sum(X, axis=0, keepdims=True)\n",
    "class_prob = np.mean(y, axis=0, keepdims=True)\n",
    "expected_table = np.dot(class_prob.T, feature_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classical statistics, hypothesis tests typically have a quantity called\n",
    "a test statistic associated with them. If the test statistic is greater than\n",
    "a critical value the hypothesis is rejected. In this case, the test statistic is\n",
    "identical with our measure of distinctiveness. The test commonly used to analyze\n",
    "the present hypothesis (that two distinct groups are unnecessary) is the log\n",
    "likelihood ratio test, and its statistic is called the log likelihood ratio\n",
    "(alternatively a [G-test](http://en.wikipedia.org/wiki/G-test) statistic or\n",
    "[Dunning log likelihood](http://acl.ldc.upenn.edu/J/J93/J93-1003.pdf)\n",
    "[[Dun93]](references.html#dunning-accurate-1993)).  Various symbols are associated with this\n",
    "statistic, including $ G $, $ G^2 $, $ l $,  and $ \\lambda $.\n",
    "(The theoretical underpinnings of the log likelihood ratio test and its\n",
    "application to corpus analysis are covered in chapter 8 of Casella and Berger\n",
    "(2001) and Dunning (1993) [[CB01]](references.html#casella-statistical-2001)\n",
    "[[Dun93]](references.html#dunning-accurate-1993).)\n",
    "\n",
    "The log likelihood ratio is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\sum_i O_i \\times \\ln \\frac{O_i}{E_i}\n",
    "$$\n",
    "\n",
    "where $ i $ indexes the cells. (Note the similarity of this formula to the\n",
    "calculation of [mutual information](#mutual-information).) In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'green_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6fdd58903bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreen_table\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreen_table\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mexpected_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'green_table' is not defined"
     ]
    }
   ],
   "source": [
    "G = np.sum(green_table * np.log(green_table / expected_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the value of the test statistic, the more pronounced the deviation is\n",
    "from the hypothesis—and, for our purposes, the more “distinctive” the word is.\n",
    "\n",
    "Pearson’s $ \\chi^2 $ test statistic approximates the log likelihood ratio\n",
    "test ($ \\chi^2 $ is read chi-squared). It is computationally easier to\n",
    "calculate. The Python library `scikit-learn` provides a function\n",
    "`sklearn.feature_selection.chi2` that allows us to use this test statistic as\n",
    "a feature selection method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filenames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-be3a37211b2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchi2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Austen\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"Austen\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"CBrontë\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filenames' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "labels = []\n",
    "for fn in filenames:\n",
    "    label = \"Austen\" if \"Austen\" in fn else \"CBrontë\"\n",
    "    labels.append(label)\n",
    "\n",
    "# chi2 returns two arrays, the chi2 test statistic and an\n",
    "# array of \"p-values\", which we'll ignore\n",
    "keyness, _ = chi2(dtm, labels)\n",
    "ranking = np.argsort(keyness)[::-1]\n",
    "vocab[ranking][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**\n",
    ">\n",
    ">Logarithms are expensive. Calculating the log likelihood ratio over\n",
    "a vocabulary of 10,000 words will involve taking 40,000 logarithms. The\n",
    "$ \\chi^2 $ test statistic, by contrast, involves taking the square of\n",
    "a quantity the same number of times. On my computer, calculating the\n",
    "logarithm takes about twenty times longer than taking the square (simple\n",
    "multiplication):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hautakivi/.pyenv/versions/3.6.1/lib/python3.6/timeit.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  times.  See also Tim Peters' introduction to the Algorithms chapter in\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.888908325055984"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "time_log = timeit.timeit(\"import numpy as np; np.log(np.arange(40000))\", number=100)\n",
    "time_square = timeit.timeit(\"import numpy as np; np.square(np.arange(40000))\", number=100)\n",
    "time_log / time_square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='mutual-information'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information feature selection\n",
    "\n",
    "Feature selection based on mutual information also delivers good results.\n",
    "Good introductions to the method can be found in [Cosma Shalizi’s Data Mining\n",
    "course](http://www.stat.cmu.edu/~cshalizi/350/) ([Finding Informative Features](http://www.stat.cmu.edu/~cshalizi/350/lectures/05/lecture-05.pdf)) and in\n",
    "[section 13.5](http://www-nlp.stanford.edu/IR-book/html/htmledition/feature-selection-1.html)\n",
    "in [[MRS08]](references.html#manning-introduction-2008)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection as exploratory data analysis\n",
    "\n",
    "If nothing else, studying methods of feature selection forces us to think\n",
    "critically about what we mean when we say some characteristic is “distinctive”.\n",
    "\n",
    "In practice, these methods let us quickly identify features (when they exist)\n",
    "that appear more or less often in one group of texts.  As such, these methods\n",
    "are useful for dimensionality reduction and exploratory data analysis.  For\n",
    "example, if we suspect that there is a meaningful partition of a collection of\n",
    "texts, we can use one of the methods described above to pull out features that\n",
    "characterize the proposed groups of texts and explore whether those features\n",
    "make sense given other information. Or we may be confronted with a massive\n",
    "dataset—perhaps all 1-, 2-, and 3-grams in the corpus—and need to reduce the\n",
    "space of features so that our analyses can run on a computer with limited\n",
    "memory.\n",
    "\n",
    "Feature selection needs to be used with care when working with a small number of\n",
    "observations and a relatively large number of features—e.g., a corpus with of\n",
    "a small number of documents and a very large vocabulary. Feature selection is\n",
    "perfectly capable of pulling out features that are characteristic of any\n",
    "division of texts.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">The shorthand $ n << p $ is used to describe situations where\n",
    "the number of variables greatly outnumbers the number observations.\n",
    "$ n $ is the customary label for the number of observations and\n",
    "$ p $ refers to the number of covariates.\n",
    "\n",
    "A brief demonstration that feature selection “works” as expected can be seen by\n",
    "plotting the cosine distance among texts in the corpus before and after feature\n",
    "selection is applied. `chi2` is the feature selection used in the bottom\n",
    "figure and the top 50 words are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Using the two groups of texts (Austen and C. Brontë), find the top 40\n",
    "  characteristic words by the $ \\chi^2 $ statistic. Feel free to use\n",
    "  scikit-learn’s `chi2`.  \n",
    "1. The following is a random partition of the texts. Find the top 40\n",
    "  characteristic words by the $ \\chi^2 $ statistic. How do these\n",
    "  compare with those you found in exercise 1?  \n",
    "\n",
    "\n",
    "1. Reconstruct the corpus using only these 40 words. Find the cosine distances\n",
    "  between pairs of texts and visualize these using multi-dimensional scaling\n",
    "  (see [Working with text](working_with_text.html#working-with-text) for a refresher). Compare this plot to the MDS\n",
    "  plot of the distances between texts using the full vocabulary.  \n",
    "\n",
    "\n",
    "<p><a id=fn-lyon href=#fn-lyon-link><strong>[1]</strong></a> Unexpected spikes in word use happen all the time. Word usage in a large corpus\n",
    "is notoriously *bursty* [[CG95]](references.html#church-poisson-1995).\n",
    "Consider, for example, ten French novels, one of which is set in Lyon.\n",
    "While “Lyon” might appear in all novels, it would appear much (much) more\n",
    "frequently in the novel set in the city.]\n",
    "\n",
    "<p><a id=fn-underwood href=#fn-underwood-link><strong>[2]</strong></a> Ted Underwood has written a [blog post discussing some of the\n",
    "drawbacks of using the log likelihood and chi-squared test statistic in the\n",
    "context of literary studies](http://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/).]"
   ]
  }
 ],
 "metadata": {
  "date": 1577189965.5303638,
  "download_nb": true,
  "download_nb_path": null,
  "filename": "feature_selection.rst",
  "filename_with_path": "feature_selection",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "title": "Feature selection: finding distinctive words"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
