{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='index-0'></a>\n",
    "\n",
    "<a id='topic-model-mallet'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling with MALLET\n",
    "\n",
    "This section illustrates how to use [MALLET](http://mallet.cs.umass.edu/) to\n",
    "model a corpus of texts using a topic model and how to analyze the results using\n",
    "Python.\n",
    "\n",
    "A topic model is a probabilistic model of the words appearing in a corpus of\n",
    "documents.  (There are a number of general introductions to topic models\n",
    "available, such as [[Ble12]](references.html#blei-introduction-2012).) The particular topic model\n",
    "used in this section is Latent Dirichlet Allocation (LDA), a model introduced in\n",
    "the context of text analysis in 2003 [[BNJ03]](references.html#blei-latent-2003). LDA is an\n",
    "instance of a more general class of models called mixed-membership models. While\n",
    "LDA involves a greater number of distributions and parameters than the Bayesian\n",
    "model introduced in the section on [group comparison](feature_selection.html#bayesian-group-comparison), both are instances of a Bayesian probabilistic\n",
    "model. In fact, posterior inference for both models is typically performed in\n",
    "precisely the same manner, using Gibbs sampling with conjugate priors.\n",
    "\n",
    "This section assumes prior exposure to topic modeling and proceeds as follows:\n",
    "\n",
    "1. MALLET is downloaded and used to fit a topic model of six novels, three by\n",
    "  Brontë and three by Austen. Because these are lengthy texts, the novels are split\n",
    "  up into smaller sections—a preprocessing step which improves results considerably.  \n",
    "1. The output of MALLET is loaded into Python as a document-topic matrix (a\n",
    "  2-dimensional array) of topic shares.  \n",
    "1. Topics, discrete distributions over the vocabulary, are analyzed.  \n",
    "\n",
    "\n",
    "Note that [an entire section](topic_model_visualization.html#topic-model-visualization) is devoted to\n",
    "visualizing topic models. This section focuses on using MALLET and processing\n",
    "the results.\n",
    "\n",
    "This section uses six novels by Brontë and Austen. These novels are divided into\n",
    "parts as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "CORPUS_PATH = os.path.join('data', 'austen-brontë-split')\n",
    "filenames = sorted([os.path.join(CORPUS_PATH, fn) for fn in os.listdir(CORPUS_PATH)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# files are located in data/austen-brontë-split\n",
    "len(filenames)\n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running MALLET\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">The `nltk` package provides a thin wrapper for MALLET which may be\n",
    "worth investigating. See `nltk.classify.mallet`.\n",
    "\n",
    "On Linux and BSD-based systems (such as OS X), the following commands should\n",
    "download and extract MALLET:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide-output": false
   },
   "source": [
    "```bash\n",
    "# alternatively: wget http://mallet.cs.umass.edu/dist/mallet-2.0.7.tar.gz\n",
    "curl --remote-name http://mallet.cs.umass.edu/dist/mallet-2.0.7.tar.gz\n",
    "tar zxf mallet-2.0.7.tar.gz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run MALLET using the default parameters. Using the option\n",
    "`--random-seed 1` should guarantee that the results produced match those\n",
    "appearing below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide-output": false
   },
   "source": [
    "```bash\n",
    "mallet-2.0.7/bin/mallet import-dir --input data/austen-brontë-split/ --output /tmp/topic-input-austen-brontë.mallet --keep-sequence --remove-stopwords\n",
    "mallet-2.0.7/bin/mallet train-topics --input /tmp/topic-input-austen-brontë.mallet --num-topics 20 --output-doc-topics /tmp/doc-topics-austen-brontë.txt --output-topic-keys /tmp/topic-keys-austen-brontë.txt --random-seed 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under Windows the commands are similar. For detailed instructions see the\n",
    "article [“Getting Started with Topic Modeling and MALLET”](http://programminghistorian.org/lessons/topic-modeling-and-mallet).  The\n",
    "MALLET homepage also has [instructions on how to install and run the software\n",
    "under Windows](http://mallet.cs.umass.edu/download.php)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing MALLET output\n",
    "\n",
    "We have already seen that [a document-term matrix is a convenient way to\n",
    "represent the word frequencies](working_with_text.html#working-with-text) associated with each\n",
    "document. Similarly, as each document is associated with a set of topic shares,\n",
    "it will be useful to gather these features into a document-topic\n",
    "matrix.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Topic shares are also referred to as topic *weights*,\n",
    "*mixture weights*, or *component weights*. Different communities favor\n",
    "different terms.\n",
    "\n",
    "Manipulating the output of MALLET into a document-topic matrix is not\n",
    "entirely intuitive. Fortunately the tools required for the job are available in\n",
    "Python and the procedure is similar to that reviewed in the previous section on\n",
    "[grouping texts](preprocessing.html#grouping-texts).\n",
    "\n",
    "MALLET delivers the topic shares for each document into a file specified by the\n",
    "`--output-doc-topics` option. In this case we have provided the output\n",
    "filename `/tmp/doc-topics-austen-brontë.txt`. The first lines of this file\n",
    "should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "#doc name topic proportion ...\n",
    "0    file:/.../austen-brontë-split/Austen_Pride0103.txt      3       0.2110215053763441      14      0.13306451612903225\n",
    "1    file:/.../austen-brontë-split/Austen_Pride0068.txt      17      0.19915254237288135     3       0.14548022598870056\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two columns of `doc-topics.txt` record the document number\n",
    "(0-based indexing) and the full path to the filename. The rest of the columns are best\n",
    "considered as (topic-number, topic-share) pairs. There are as many of these\n",
    "pairs as there are topics. All columns are separated by tabs (there’s even\n",
    "a trailing tab at the end of the line). With the exception of the header (the\n",
    "first line), this file records data using [tab-separated values](https://en.wikipedia.org/wiki/Tab-separated_values). There are two challenges\n",
    "in parsing this file into a document-topic matrix. The first is sorting.\n",
    "The texts do not appear in a consistent order in `doc-topics.txt` and the\n",
    "topic number and share pairs appear in different columns depending on the\n",
    "document. We will need to reorder these pairs before assembling them into\n",
    "a matrix.[#fnmapreduce]_ The second challenge is that the number of columns will\n",
    "vary with the number of topics specified (`--num-topics`). Fortunately, the\n",
    "documentation in the Python library [itertools](http://docs.python.org/dev/library/itertools.html) describes a function\n",
    "called `grouper` using `itertools.izip_longest` that solves our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "import os\n",
    "\n",
    "def grouper(n, iterable, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "doctopic_triples = []\n",
    "mallet_docnames = []\n",
    "\n",
    "with open(\"/tmp/doc-topics-austen-brontë.txt\") as f:\n",
    "    f.readline()  # read one line in order to skip the header\n",
    "    for line in f:\n",
    "        # ``docnum, docname, *values`` performs \"tuple unpacking\", useful Python feature\n",
    "        # ``.rstrip()`` removes the superfluous trailing tab\n",
    "        docnum, docname, *values = line.rstrip().split('\\t')\n",
    "        mallet_docnames.append(docname)\n",
    "        for topic, share in grouper(2, values):\n",
    "            triple = (docname, int(topic), float(share))\n",
    "            doctopic_triples.append(triple)\n",
    "\n",
    "# sort the triples\n",
    "# triple is (docname, topicnum, share) so sort(key=operator.itemgetter(0,1))\n",
    "# sorts on (docname, topicnum) which is what we want\n",
    "doctopic_triples = sorted(doctopic_triples, key=operator.itemgetter(0,1))\n",
    "\n",
    "# sort the document names rather than relying on MALLET's ordering\n",
    "mallet_docnames = sorted(mallet_docnames)\n",
    "\n",
    "# collect into a document-term matrix\n",
    "num_docs = len(mallet_docnames)\n",
    "num_topics = len(doctopic_triples) // len(mallet_docnames)\n",
    "\n",
    "# the following works because we know that the triples are in sequential order\n",
    "doctopic = np.zeros((num_docs, num_topics))\n",
    "for triple in doctopic_triples:\n",
    "    docname, topic, share = triple\n",
    "    row_num = mallet_docnames.index(docname)\n",
    "    doctopic[row_num, topic] = share\n",
    "\n",
    "@suppress\n",
    "doctopic_orig = doctopic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# The following method is considerably faster. It uses the itertools library which is part of the Python standard library.\n",
    "import itertools\n",
    "import operator\n",
    "doctopic = np.zeros((num_docs, num_topics))\n",
    "for i, (doc_name, triples) in enumerate(itertools.groupby(doctopic_triples, key=operator.itemgetter(0))):\n",
    "    doctopic[i, :] = np.array([share for _, _, share in triples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the average of the topic shares associated with each\n",
    "novel. Recall that we have been working with small sections of novels. The\n",
    "following step combines the topic shares for sections associated with the same\n",
    "novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "novel_names = []\n",
    "for fn in filenames:\n",
    "    basename = os.path.basename(fn)\n",
    "    # splitext splits the extension off, 'novel.txt' -> ('novel', '.txt')\n",
    "    name, ext = os.path.splitext(basename)\n",
    "    # remove trailing numbers identifying chunk\n",
    "    name = name.rstrip('0123456789')\n",
    "    novel_names.append(name)\n",
    "# turn this into an array so we can use NumPy functions\n",
    "novel_names = np.asarray(novel_names)\n",
    "\n",
    "@suppress\n",
    "assert len(set(novel_names)) == 6\n",
    "@supress\n",
    "doctopic_orig = doctopic.copy()\n",
    "\n",
    "# use method described in preprocessing section\n",
    "num_groups = len(set(novel_names))\n",
    "doctopic_grouped = np.zeros((num_groups, num_topics))\n",
    "for i, name in enumerate(sorted(set(novel_names))):\n",
    "    doctopic_grouped[i, :] = np.mean(doctopic[novel_names == name, :], axis=0)\n",
    "\n",
    "doctopic = doctopic_grouped\n",
    "\n",
    "@suppress\n",
    "docnames = sorted(set(novel_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit into the space available, the table above displays the first 15\n",
    "of 20 topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the topic model\n",
    "\n",
    "The first thing we should appreciate about our topic model is that the twenty\n",
    "shares do a remarkably good job of summarizing our corpus. For example, they\n",
    "preserve the distances between novels (see figures below). By this measure, LDA\n",
    "is good at dimensionality reduction: we have taken a matrix of dimensions 813 by\n",
    "14862 (occupying almost three megabytes of memory if stored in a spare matrix)\n",
    "and fashioned a representation that preserves important features in a matrix\n",
    "that is 813 by 20 (5% the size of the original).\n",
    "\n",
    "Even though a topic model “discards” the “fine-grained” information recorded in\n",
    "the matrix of word frequencies, it preserves salient details of the underlying\n",
    "matrix. That is, the topic shares associated with a document have an\n",
    "interpretation in terms of word frequencies. This is best illustrated by\n",
    "examining the present topic model.\n",
    "\n",
    "First let us identify the most significant topics for each text in the corpus.\n",
    "This procedure does not differ in essence from the procedure for identifying the\n",
    "most frequent words in each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "novels = sorted(set(novel_names))\n",
    "print(\"Top topics in...\")\n",
    "for i in range(len(doctopic)):\n",
    "    top_topics = np.argsort(doctopic[i,:])[::-1][0:3]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: {}\".format(novels[i], top_topics_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**\n",
    ">\n",
    ">Recall that, like everything else in Python (and C, Java, and many\n",
    "other languages), the topics use 0-based indexing; the first topic is topic 0.\n",
    "\n",
    "Each topic in the topic model can be inspected. Each topic is a distribution\n",
    "which captures in probabilistic terms, the words associated with the topic and\n",
    "the strength of the association (the posterior probability of finding a word\n",
    "associated with a topic). Sometimes this distribution is called a topic-word\n",
    "distribution (in contrast to the document-topic distribution). Again, this is\n",
    "best illustrated by inspecting the topic-word distributions provided by MALLET\n",
    "for our Austen-Brontë corpus.  MALLET places (a subset of) the topic-word\n",
    "distribution for each topic in a file specified by the command-line option\n",
    "`--output-topic-keys`. For the run of `mallet` used in this section, this\n",
    "file is `/tmp/topic-keys-austen-brontë.txt`. The first line of this file\n",
    "should resemble the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "0    2.5     long room looked day eyes make voice head till girl morning feel called table turn continued times appeared breakfast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to parse this file into something we can work with. Fortunately this\n",
    "task is not difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "with open('/tmp/topic-keys-austen-brontë.txt') as input:\n",
    "    topic_keys_lines = input.readlines()\n",
    "topic_words = []\n",
    "for line in topic_keys_lines:\n",
    "    _, _, words = line.split('\\t')  # tab-separated\n",
    "    words = words.rstrip().split(' ')  # remove the trailing '\\n'\n",
    "    topic_words.append(words)\n",
    "\n",
    "# now we can get a list of the top words for topic 0 with topic_words[0]\n",
    "topic_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to list the words associated with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N_WORDS_DISPLAY = 10\n",
    "for t in range(len(topic_words)):\n",
    "    print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:N_WORDS_DISPLAY])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to inspect and to visualize topic models. Some of the more\n",
    "common methods are covered in [next section](topic_model_visualization.html#topic-model-visualization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinctive topics\n",
    "\n",
    "Finding distinctive topics is analogous to the task of [finding distinctive\n",
    "words](feature_selection.html#feature-selection). The topic model does an excellent job of focusing\n",
    "attention on recurrent patterns (of co-occurrence) in the word frequencies\n",
    "appearing in a corpus. To the extent that we are interested in these kinds of\n",
    "patterns (rather than the rare or isolated feature of texts), working with\n",
    "topics tends to be easier than working with word frequencies.\n",
    "\n",
    "Consider the task of finding the distinctive topics in Austen’s novels. Here the\n",
    "simple difference-in-averages provides an easy way of finding topics that tend\n",
    "to be associated more strongly with Austen’s novels than with Brontë’s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "austen_indices, cbronte_indices = [], []\n",
    "for index, fn in enumerate(sorted(set(novel_names))):\n",
    "    if \"Austen\" in fn:\n",
    "        austen_indices.append(index)\n",
    "    elif \"CBronte\" in fn:\n",
    "        cbronte_indices.append(index)\n",
    "\n",
    "austen_avg = np.mean(doctopic[austen_indices, :], axis=0)\n",
    "cbronte_avg = np.mean(doctopic[cbronte_indices, :], axis=0)\n",
    "keyness = np.abs(austen_avg - cbronte_avg)\n",
    "ranking = np.argsort(keyness)[::-1]  # from highest to lowest; [::-1] reverses order in Python sequences\n",
    "\n",
    "# distinctive topics:\n",
    "ranking[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a id=fnmapreduce href=#fnmapreduce-link><strong>[1]</strong></a> Those familiar with [MapReduce](https://en.wikipedia.org/wiki/MapReduce) may recognize the pattern of splitting a dataset into smaller pieces and then (re)ordering them."
   ]
  }
 ],
 "metadata": {
  "date": 1577189965.9675195,
  "filename": "topic_model_mallet.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Topic modeling with MALLET"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}