<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">

		<title>Classification, Machine Learning, and Logistic Regression &ndash; Documentation</title>

		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="author" content="">
		<meta name="keywords" content="">
		<meta name="description" content="">

		<link rel="stylesheet" href="/_static/css/base.css">

	</head>

	<body>

		<div class="wrapper">

			<header class="header">

				<div class="branding">

					<p class="site-title"><a href="/">Documentation</a></p>

					<p class="sr-only"><a href="#skip">Skip to content</a></p>

				</div>

				<div class="header-tools">


					<div class="header-badge" id="executability_status_badge"></div>


				</div>

			</header>

			<div class="main">

				<div class="content">

					<div id="skip"></div>

					<div class="document">

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a id='index-0'></a></p>
<p><a id='classification-machine-learning'></a></p>

</div>
</div>
</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Classification,-Machine-Learning,-and-Logistic-Regression">Classification, Machine Learning, and Logistic Regression<a class="anchor-link" href="#Classification,-Machine-Learning,-and-Logistic-Regression">&#182;</a></h1><p>Previous tutorials have illustrated how probabilistic topic models can be used
to navigate a large corpus. As general purpose tools for identifying recurrent
themes in a corpus, topic models and non-negative matrix factorization are
useful. They perform better than methods previously used for similar
purposes, such as principle component analysis (PCA) and latent semantic
analysis (LSA). For tasks such as classifying texts into a known set of categories, however,
there exist methods that are better suited to the problem. One family of such methods
goes under the heading of neural networks (or, more recently, “deep learning”).
An essential conceptual and practical building block for these methods is
logistic regression, which we will review briefly in this tutorial.</p>
<blockquote><p><strong>Note</strong></p>
<p>Discussion of the role of logistic regression in neural networks may
be found in section 5.1 of Bishop (2007) <a href="references.html#bishop-pattern-2007">[Bis07]</a>.</p>
</blockquote>

</div>
</div>
</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Predicting-genre-classifications">Predicting genre classifications<a class="anchor-link" href="#Predicting-genre-classifications">&#182;</a></h2><p>The bag-of-words model is a horrible model of a text. Its failure to distinguish
word order (‘the cat ate the fish’ from ‘the fish ate the cat’) is the least of
its failings. In most cases, knowing the frequency with which a word occurs in
a text tells us very little. Without additional context it is
difficult to know how to interpret a word’s frequency. For example, the word
‘heart’ might occur in a discussion of courtly love, of physical exercise, or in
a cookbook (e.g., “heart of palm”). And even when a word seems to have a single
interpretation, its meaning may depend on words occurring around it.</p>
<p>Nevertheless, sometimes the frequency of words appears to be correlated with
useful information, such as pre-existing classifications (or classifications in
which we happen to believe). Consider the word “ennemis” (“enemies”) in the
context of a corpus of <a href="datasets.html#datasets">French classical theatre</a>. This corpus
includes only plays classified as tragedy or comedy. The word “ennemis” is not,
at first glance, a word particularly troubled by problems of polysemy.
Considered as an indicator of whether or not a play is a tragedy or a comedy,
the frequency of “ennemis” seems to be a reliable guide; the word tends to occur
more often in tragedies.</p>
<p>The first way we can verify this is simply to calculate the percentage of plays
classified as tragedy in which the word ‘ennemis’ occurs and compare that
percentage with the corresponding percentage for comedies. As usual, in order to
have a better sense of the variability of language in French classical theatre,
we have split the plays into approximately 1,000-word sections.</p>

</div>
</div>
</div>

<div class="{} cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
	<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;data/french-tragedies-and-comedies-split/&#39;</span>

<span class="n">filenames</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">data_dir</span><span class="p">))</span>
<span class="n">filenames_with_path</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span> <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">]</span>

<span class="c1"># tragedies and comedies are coded with &#39;TR&#39; or &#39;CO&#39;,</span>
<span class="c1"># e.g., PCorneille_TR-V-1647-Heraclius0001.txt</span>
<span class="n">genre</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">:</span>
    <span class="n">genre</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;tragedy&#39;</span> <span class="k">if</span> <span class="s1">&#39;_TR-&#39;</span> <span class="ow">in</span> <span class="n">fn</span> <span class="k">else</span> <span class="s1">&#39;comedy&#39;</span><span class="p">)</span>
<span class="n">genre</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">genre</span><span class="p">)</span>

<span class="c1"># .strip() removes the trailing newline &#39;\n&#39; from each line in the file</span>
<span class="n">french_stopwords</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/stopwords/french.txt&#39;</span><span class="p">)]</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s1">&#39;filename&#39;</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=.</span><span class="mi">95</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="n">french_stopwords</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>
<span class="n">dtm</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">filenames_with_path</span><span class="p">)</span>
<span class="n">dtm</span> <span class="o">=</span> <span class="n">dtm</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>

<span class="c1"># texts are split into documents of approximately equal length, so we will</span>
<span class="c1"># skip the normalization step and deal directly with counts</span>
</pre></div>

	</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

	<div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">FileNotFoundError</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-1-dd2c30107ea5&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> data_dir <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">&#39;data/french-tragedies-and-comedies-split/&#39;</span>
<span class="ansi-green-intense-fg ansi-bold">      7</span> 
<span class="ansi-green-fg">----&gt; 8</span><span class="ansi-red-fg"> </span>filenames <span class="ansi-blue-fg">=</span> np<span class="ansi-blue-fg">.</span>array<span class="ansi-blue-fg">(</span>os<span class="ansi-blue-fg">.</span>listdir<span class="ansi-blue-fg">(</span>data_dir<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      9</span> filenames_with_path <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span>os<span class="ansi-blue-fg">.</span>path<span class="ansi-blue-fg">.</span>join<span class="ansi-blue-fg">(</span>data_dir<span class="ansi-blue-fg">,</span> fn<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> fn <span class="ansi-green-fg">in</span> filenames<span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">     10</span> 

<span class="ansi-red-fg">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;data/french-tragedies-and-comedies-split/&#39;</pre>
</div>
</div>

</div>
</div>

</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Having assembled the corpus, it is easy to calculate the number of play sections
in which ‘ennemis’ occurs.</p>

</div>
</div>
</div>

<div class="{} cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
	<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">word</span> <span class="o">=</span> <span class="s2">&quot;ennemis&quot;</span>
<span class="n">tragedy_counts</span> <span class="o">=</span> <span class="n">dtm</span><span class="p">[</span><span class="n">genre</span> <span class="o">==</span> <span class="s1">&#39;tragedy&#39;</span><span class="p">,</span> <span class="n">vocab</span> <span class="o">==</span> <span class="n">word</span><span class="p">]</span>
<span class="n">comedy_counts</span> <span class="o">=</span> <span class="n">dtm</span><span class="p">[</span><span class="n">genre</span> <span class="o">==</span> <span class="s1">&#39;comedy&#39;</span><span class="p">,</span> <span class="n">vocab</span> <span class="o">==</span> <span class="n">word</span><span class="p">]</span>

<span class="c1"># tragedy percentage</span>
<span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">tragedy_counts</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">tragedy_counts</span><span class="p">)</span>
<span class="c1"># comedy percentage</span>
<span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">comedy_counts</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">comedy_counts</span><span class="p">)</span>

<span class="c1"># overall percentage</span>
<span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">dtm</span><span class="p">[:,</span> <span class="n">vocab</span> <span class="o">==</span> <span class="n">word</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dtm</span><span class="p">)</span>

<span class="c1"># text in which &quot;ennemis&quot; appears the most</span>
<span class="n">filenames</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dtm</span><span class="p">[:,</span> <span class="n">vocab</span> <span class="o">==</span> <span class="n">word</span><span class="p">])],</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dtm</span><span class="p">[:,</span> <span class="n">vocab</span> <span class="o">==</span> <span class="n">word</span><span class="p">])</span>
</pre></div>

	</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

	<div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-2-4ad75a68675f&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> word <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">&#34;ennemis&#34;</span>
<span class="ansi-green-fg">----&gt; 2</span><span class="ansi-red-fg"> </span>tragedy_counts <span class="ansi-blue-fg">=</span> dtm<span class="ansi-blue-fg">[</span>genre <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;tragedy&#39;</span><span class="ansi-blue-fg">,</span> vocab <span class="ansi-blue-fg">==</span> word<span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> comedy_counts <span class="ansi-blue-fg">=</span> dtm<span class="ansi-blue-fg">[</span>genre <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;comedy&#39;</span><span class="ansi-blue-fg">,</span> vocab <span class="ansi-blue-fg">==</span> word<span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> 
<span class="ansi-green-intense-fg ansi-bold">      5</span> <span class="ansi-red-fg"># tragedy percentage</span>

<span class="ansi-red-fg">NameError</span>: name &#39;dtm&#39; is not defined</pre>
</div>
</div>

</div>
</div>

</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In our sample, if a play section is a tragedy it features the word ‘ennemis’ about a third
of time. Among comedy sections, the word appears in only five percent. (Recall, however,
that in the majority of play sections the word <em>does not appear</em> at all.) While this
gives us a rough sense of the relationship between the word ‘ennemis’ and genre,
we may want to describe the relationship more precisely.  First, we would like to
consider the relationship between the word’s frequency (rather than just its
presence or absence) and a text’s classification. Second, we want to
predict the classification of a section of a play for which we do not have
a classification ready at hand. Logistic regression accomplishes both of these
tasks.</p>
<p>Like linear regression, logistic regression will happily make predictions based
on aleatory patterns in our data. It is therefore important to make sure we have
some additional basis for believing there might be a correlation between the
frequency of the word ‘ennemis’ and a genre classification. Our intuition tells
us that the word (particularly in its plural form) does not belong in a comedy
(or at least not in any great frequency), whereas we can imagine a variety of
sentences using the word appearing in a tragedy.  Consider, for example, the
section of Racine’s <em>Thebaide</em> which features the six occurrences of the word
(and plenty of ‘ennemi’ as well):</p>

</div>
</div>
</div>

<div class="{} cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
	<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Plus</span> <span class="n">qu</span><span class="s1">&#39;à mes ennemis la guerre m&#39;</span><span class="n">est</span> <span class="n">mortelle</span><span class="p">,</span>
<span class="n">Et</span> <span class="n">le</span> <span class="n">courroux</span> <span class="n">du</span> <span class="n">ciel</span> <span class="n">me</span> <span class="n">la</span> <span class="n">rend</span> <span class="n">trop</span> <span class="n">cruelle</span> <span class="p">;</span>
<span class="n">Il</span> <span class="n">s</span><span class="s1">&#39;arme contre moi de mon propre dessein,</span>
<span class="n">Il</span> <span class="n">se</span> <span class="n">sert</span> <span class="n">de</span> <span class="n">mon</span> <span class="n">bras</span> <span class="n">pour</span> <span class="n">me</span> <span class="n">percer</span> <span class="n">le</span> <span class="n">sein</span><span class="o">.</span>
<span class="n">La</span> <span class="n">guerre</span> <span class="n">s</span><span class="s1">&#39;allumait, lorsque pour mon supplice,</span>
<span class="n">Hémon</span> <span class="n">m</span><span class="s1">&#39;abandonna pour servir Polynice ;</span>
<span class="n">Les</span> <span class="n">deux</span> <span class="n">frères</span> <span class="n">par</span> <span class="n">moi</span> <span class="n">devinrent</span> <span class="n">ennemis</span><span class="p">,</span>
<span class="n">Et</span> <span class="n">je</span> <span class="n">devins</span><span class="p">,</span> <span class="n">Attale</span><span class="p">,</span> <span class="n">ennemi</span> <span class="n">de</span> <span class="n">mon</span> <span class="n">fils</span><span class="o">.</span>
<span class="o">...</span>
</pre></div>

	</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

	<div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-cyan-fg">  File </span><span class="ansi-green-fg">&#34;&lt;ipython-input-3-689da7becab0&gt;&#34;</span><span class="ansi-cyan-fg">, line </span><span class="ansi-green-fg">1</span>
<span class="ansi-red-fg">    Plus qu&#39;à mes ennemis la guerre m&#39;est mortelle,</span>
          ^
<span class="ansi-red-fg">SyntaxError</span><span class="ansi-red-fg">:</span> invalid syntax
</pre>
</div>
</div>

</div>
</div>

</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In quantitative text analysis, a common way to represent a classification is as
a binary outcome, e.g., 0 for comedy or 1 for tragedy. Whereas linear regression
relates some quantity <code>x</code> to another quantity <code>y</code>, logistic regression
relates a quantity <code>x</code> to the <em>probability</em> of something being a member of one
of two groups, that is, the probability of <code>y</code> having a value of 1.</p>
<p>For reasons covered in greater detail at the <a href="#logistic-regression">end of this section</a>, the probability of classification is expressed not in
terms of probability (from 0 to 1) but in log <a href="https://en.wikipedia.org/wiki/Odds">odds</a>. This is not a mysterious transformation.
Indeed, in certain countries (and among individuals involved in
gambling) expressing the likelihood of an event in terms of odds is common.
Moving between probability, odds, and log odds is somewhat tedious but not
difficult—e.g., an event occurring with probability 0.75, it occurs with odds
3 (often expressed 3:1) and with log odds 1.1. Logistic regression delivers, for
any value of <code>x</code>, here the frequency of the word ‘ennemis’, the log odds of
a play section being from a tragedy.  Typically we immediately convert the log
odds into probability as the latter is more familiar.</p>
<blockquote><p><strong>Note</strong></p>
<p>For very rare or very probable events using odds (and even log
odds) can be preferable to using probabilities. Consider the
<a href="https://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change">Intergovernmental Panel on Climate Change’s</a>
<a href="https://www.ipcc.ch/pdf/supporting-material/uncertainty-guidance-note_ar4.pdf">guidance on addressing uncertainties</a>.</p>
<table>
<thead><tr>
<th style="text-align:center">Terminology</th>
<th style="text-align:center">Likelihood</th>
<th style="text-align:center">Odds</th>
<th style="text-align:center">Log odds</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Virtually certain</td>
<td style="text-align:center">99% probability</td>
<td style="text-align:center">99 (or 99:1)</td>
<td style="text-align:center">&gt; 4.6</td>
</tr>
<tr>
<td style="text-align:center">Very likely</td>
<td style="text-align:center">&gt; 90% probability</td>
<td style="text-align:center">&gt; 9</td>
<td style="text-align:center">&gt; 2.2</td>
</tr>
<tr>
<td style="text-align:center">Likely</td>
<td style="text-align:center">&gt; 66% probability</td>
<td style="text-align:center">&gt; 2</td>
<td style="text-align:center">&gt; 0.7</td>
</tr>
<tr>
<td style="text-align:center">About as likely as not</td>
<td style="text-align:center">33 to 66% probability</td>
<td style="text-align:center">0.5 to 2</td>
<td style="text-align:center">-0.7 to 0.7</td>
</tr>
<tr>
<td style="text-align:center">Unlikely</td>
<td style="text-align:center">&lt; 33% probability</td>
<td style="text-align:center">&lt; 0.5</td>
<td style="text-align:center">&lt; -0.7</td>
</tr>
<tr>
<td style="text-align:center">Very unlikely</td>
<td style="text-align:center">&lt; 10% probability</td>
<td style="text-align:center">&lt; .1</td>
<td style="text-align:center">&lt; -2.2</td>
</tr>
<tr>
<td style="text-align:center">Exceptionally unlikely</td>
<td style="text-align:center">&lt; 1% probability</td>
<td style="text-align:center">&lt; 0.01</td>
<td style="text-align:center">&lt; -4.6</td>
</tr>
</tbody>
</table>
</blockquote>
<p>Note that whereas moving from a likelihood of 33% to 66% corresponds to
moving from 0.5 to 2 on the odds scale, moving from 90% to 99% entails
moving from 9 to 99 on the odds scale. The odds scale expresses better
the difference between an event that happens 9 out of 10 times versus an
event that happens 99 times out of 100.</p>
<p>First we will fit the logistic regression model using the <code>statsmodels</code>
package and then, converting from log odds to the more familiar scale of
probability, we will plot this estimated relationship.</p>

</div>
</div>
</div>

<div class="{} cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
	<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="n">wordfreq</span> <span class="o">=</span> <span class="n">dtm</span><span class="p">[:,</span> <span class="n">vocab</span> <span class="o">==</span> <span class="s2">&quot;ennemis&quot;</span><span class="p">]</span>

<span class="c1"># we need to add an intercept (whose coefficient is related to the</span>
<span class="c1"># probability that a novel will be classified a tragedy when the</span>
<span class="c1"># predictor is zero.</span>
<span class="c1"># This is done automatically in R and by sklearn&#39;s LogisticRegression</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">wordfreq</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">genre</span> <span class="o">==</span> <span class="s1">&#39;tragedy&#39;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">fit</span><span class="o">.</span><span class="n">params</span>
</pre></div>

	</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

	<div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-4-b88411b2f2f6&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-green-fg">import</span> statsmodels<span class="ansi-blue-fg">.</span>api <span class="ansi-green-fg">as</span> sm
<span class="ansi-green-intense-fg ansi-bold">      2</span> 
<span class="ansi-green-fg">----&gt; 3</span><span class="ansi-red-fg"> </span>wordfreq <span class="ansi-blue-fg">=</span> dtm<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">:</span><span class="ansi-blue-fg">,</span> vocab <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#34;ennemis&#34;</span><span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> 
<span class="ansi-green-intense-fg ansi-bold">      5</span> <span class="ansi-red-fg"># we need to add an intercept (whose coefficient is related to the</span>

<span class="ansi-red-fg">NameError</span>: name &#39;dtm&#39; is not defined</pre>
</div>
</div>

</div>
</div>

</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For those accustomed to fitting regression models in R, the following code
produces precisely the same results:</p>

</div>
</div>
</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="nf">data.frame</span><span class="p">(</span><span class="n">wordfreq</span> <span class="o">=</span> <span class="n">wordfreq</span><span class="p">,</span> <span class="n">genre</span> <span class="o">=</span> <span class="n">genre</span> <span class="o">==</span> <span class="s">&#39;tragedy&#39;</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="nf">glm</span><span class="p">(</span><span class="n">genre</span> <span class="o">~</span> <span class="n">wordfreq</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">family</span> <span class="o">=</span> <span class="nf">binomial</span><span class="p">(</span><span class="n">link</span><span class="o">=</span><span class="s">&quot;logit&quot;</span><span class="p">))</span>
<span class="nf">coef</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span>

<span class="c1"># note that R is implicitly adding a constant term. We can make this</span>
<span class="c1"># term explicit in our model if we choose (the results should be the same)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="nf">glm</span><span class="p">(</span><span class="n">genre</span> <span class="o">~</span> <span class="m">1</span> <span class="o">+</span> <span class="n">wordfreq</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">family</span> <span class="o">=</span> <span class="nf">binomial</span><span class="p">(</span><span class="n">link</span><span class="o">=</span><span class="s">&quot;logit&quot;</span><span class="p">))</span>
<span class="nf">coef</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using the fitted parameters of the model we can make a prediction for any given
word frequency. For example, the probability of a section in which ‘ennemis’
occurs twice given by</p>

</div>
</div>
</div>

<div class="{} cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
	<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">invlogit</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert from log odds to probability&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">invlogit</span><span class="p">(</span><span class="n">fit</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">fit</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
</pre></div>

	</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

	<div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-5-fae4a6a3a9b3&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> 
<span class="ansi-green-intense-fg ansi-bold">      5</span> x <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">2</span>
<span class="ansi-green-fg">----&gt; 6</span><span class="ansi-red-fg"> </span>invlogit<span class="ansi-blue-fg">(</span>fit<span class="ansi-blue-fg">.</span>params<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">+</span> fit<span class="ansi-blue-fg">.</span>params<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">*</span> x<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;fit&#39; is not defined</pre>
</div>
</div>

</div>
</div>

</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following code plots the relationship between a section’s word frequency and
the model’s estimate of the probability of a section being from a tragedy.  The
points on the figure mark the observations in the corpus. (The points have been
jittered to improve readability.)</p>

</div>
</div>
</div>

<div class="{} cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
	<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">wordfreq</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">wordfreq</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">invlogit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># jitter the outcomes (0 or 1) a bit</span>
<span class="n">jitter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">genre</span><span class="p">))</span> <span class="o">/</span> <span class="mi">5</span>
<span class="n">ys_outcomes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">genre</span> <span class="o">==</span> <span class="s1">&#39;tragedy&#39;</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.01</span> <span class="o">-</span> <span class="n">jitter</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="c1"># use different colors for the different classes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">wordfreq</span><span class="p">[</span><span class="n">genre</span> <span class="o">==</span> <span class="s1">&#39;tragedy&#39;</span><span class="p">],</span> <span class="n">ys_outcomes</span><span class="p">[</span><span class="n">genre</span> <span class="o">==</span> <span class="s1">&#39;tragedy&#39;</span><span class="p">],</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">wordfreq</span><span class="p">[</span><span class="n">genre</span> <span class="o">!=</span> <span class="s1">&#39;tragedy&#39;</span><span class="p">],</span> <span class="n">ys_outcomes</span><span class="p">[</span><span class="n">genre</span> <span class="o">!=</span> <span class="s1">&#39;tragedy&#39;</span><span class="p">],</span> <span class="s1">&#39;y.&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Word frequency&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Predicted probability of play section being a tragedy&quot;</span><span class="p">)</span>

<span class="nd">@suppress</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">wordfreq</span><span class="p">)</span> <span class="o">==</span> <span class="mi">6</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Predicting genre by the frequency of &#39;ennemis&#39;&quot;</span><span class="p">)</span>
<span class="c1"># make some final aesthetic adjustments of the plot boundary</span>
<span class="nd">@savefig</span> <span class="n">plot_logistic_ennemis</span><span class="o">.</span><span class="n">png</span> <span class="n">width</span><span class="o">=</span><span class="mi">7</span><span class="ow">in</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">wordfreq</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>

	</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

	<div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-cyan-fg">  File </span><span class="ansi-green-fg">&#34;&lt;ipython-input-6-e2eb73dc8a7b&gt;&#34;</span><span class="ansi-cyan-fg">, line </span><span class="ansi-green-fg">15</span>
<span class="ansi-red-fg">    assert np.max(wordfreq) == 6</span>
         ^
<span class="ansi-red-fg">SyntaxError</span><span class="ansi-red-fg">:</span> invalid syntax
</pre>
</div>
</div>

</div>
</div>

</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The figure illustrates what the model infers: if ‘ennemis’ appears more than
three times in a section it will tend to be a tragedy with high probability.</p>
<p>As an experiment and an illustration of <a href="https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#K-fold_cross-validation">cross validation</a>
(also called out-of-sample validation), consider the task of predicting the
classification of a section of text based on the frequency of ‘ennemis’ alone.
From the 3,429 play sections in our corpus we will take one third of them at
random and ask the model to predict their classification with the model
fitted on the remaining sections. We will do this three times (once for each
held-out third). The scikit-learn package makes this procedure embarrassingly
easy, provided we use its version of logistic regression, which is designed for
large datasets and differs slightly from the version provided by R and
statsmodels. <sup><a href=#fn-sklearn-logisticregression id=fn-sklearn-logisticregression-link>[1]</a></sup></p>

</div>
</div>
</div>

<div class="{} cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
	<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cross_validation</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">cross_validation</span><span class="o">.</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">wordfreq</span><span class="p">,</span> <span class="n">genre</span> <span class="o">==</span> <span class="s1">&#39;tragedy&#39;</span><span class="p">)</span>
</pre></div>

	</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

	<div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ImportError</span>                               Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-7-5e09f5613b3f&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span><span class="ansi-green-fg">from</span> sklearn <span class="ansi-green-fg">import</span> cross_validation
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span class="ansi-green-fg">from</span> sklearn <span class="ansi-green-fg">import</span> linear_model
<span class="ansi-green-intense-fg ansi-bold">      3</span> 
<span class="ansi-green-intense-fg ansi-bold">      4</span> clf <span class="ansi-blue-fg">=</span> linear_model<span class="ansi-blue-fg">.</span>LogisticRegression<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> cross_validation<span class="ansi-blue-fg">.</span>cross_val_score<span class="ansi-blue-fg">(</span>clf<span class="ansi-blue-fg">,</span> wordfreq<span class="ansi-blue-fg">,</span> genre <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;tragedy&#39;</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">ImportError</span>: cannot import name &#39;cross_validation&#39;</pre>
</div>
</div>

</div>
</div>

</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since ‘ennemis’ only appears in 20% of the sections and appears more than once
in only 5% of the sections, the model will only have useful information to work
with in a fraction of the cases presented to it. Nevertheless, it does
considerably better than a baseline of simply picking ‘tragedy’ every time, which
would be expected to achieve 52% accuracy, as sections from tragedies make up 52% of the sections.</p>
<p>Of course, if we give the model access to all the word frequencies in the corpus
(not just ‘ennemis’) and ask it to make predictions it does much better:</p>

</div>
</div>
</div>

<div class="{} cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
	<div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">cross_validation</span><span class="o">.</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">dtm</span><span class="p">,</span> <span class="n">genre</span> <span class="o">==</span> <span class="s1">&#39;tragedy&#39;</span><span class="p">)</span>
</pre></div>

	</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

	<div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-8-e0e016f9ea9d&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>clf <span class="ansi-blue-fg">=</span> linear_model<span class="ansi-blue-fg">.</span>LogisticRegression<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> cross_validation<span class="ansi-blue-fg">.</span>cross_val_score<span class="ansi-blue-fg">(</span>clf<span class="ansi-blue-fg">,</span> dtm<span class="ansi-blue-fg">,</span> genre <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;tragedy&#39;</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;linear_model&#39; is not defined</pre>
</div>
</div>

</div>
</div>

</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p><strong>Note</strong></p>
<p>Those interested in using a large number of predictors—such as
a matrix with 3,000 features—should use the implementation of logistic
regression found in scikit-learn. Unlike the default version provided by
R or statsmodels, scikit-learn’s version includes a <a href="https://en.wikipedia.org/wiki/Regularization_%28mathematics%29">penalty or
regularization term</a>, which
tends to help prevent <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> that can occur in models using
a large number of predictors.</p>
</blockquote>
<p><a id='logistic-regression'></a></p>

</div>
</div>
</div>

<div class="{} cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Logistic-regression">Logistic regression<a class="anchor-link" href="#Logistic-regression">&#182;</a></h2><blockquote><p><strong>Note</strong></p>
<p>Resources for those interested in learning about logistic (and linear)
regression include Gelman and Hill (2006) <a href="references.html#gelman-data-2006">[GH06]</a> and
Bishop (2007) <a href="references.html#bishop-pattern-2007">[Bis07]</a>. Stanford’s OpenClassroom also has
a <a href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=DeepLearning">series of lectures devoted to logistic regression</a>.</p>
</blockquote>
<p>Linear regression is one way of thinking about the relationship between two
variables. Logistic regression is a linear model as well; it assumes a linear,
additive relationship between the predictors and the <em>log odds</em> of a classification.
With a single predictor and an intercept term, the relationship between
a classification and a predictor has the following symbolic expression:</p>
$$
P(y_i = \mathrm{tragedy}) &amp;= \mathrm{logit}^{-1}(\beta_0 + \beta_1 x_i)\\
           &amp;= \frac{e^{\beta_0 + \beta x_i}}{1+e^{\beta_0 + \beta_1 x_i}}\\
           &amp;= \frac{1}{1+e^{-(\beta_0 + \beta_1 x_i)}}\\
           &amp;= \sigma(\beta_0 + \beta_1 x_i)\\
$$<p>Typically we have more than one observation. Letting $ \sigma(x_i\beta) $
stand in for $ \frac{1}{1+e^{-(\beta_0 + \beta_1 x_i)}} $ the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimate">maximum
likelihood estimate</a> for $ \beta $
is the value of $ \beta $ which maximizes the log
likelihood of the observations:</p>
$$
\log \prod_{i=1}^n P(y_i = \mathrm{tragedy}) &amp;= \sum \left( y_i \log \sigma(x_i \beta) + (1 - y_i) \log (1 - \sigma(x_i \beta)) \right)\\
$$<p>While for linear regression there is frequently a closed-form solution for the
maximum, logistic regression lacks a tidy solution. The solution (there is
indeed a unique maximum) is typically found using <a href="https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares">iteratively reweighted least
squares</a>.</p>
<p>The solution may be found in Python using <code>statsmodels.api.GLM</code> or in R using
the built-in <code>glm</code> function. The two functions should yield identical results.</p>
<p><p><a id=fn-sklearn-logisticregression href=#fn-sklearn-logisticregression-link><strong>[1]</strong></a> Scikit-learn’s <code>LogisticRegression</code>
includes a penalty term which prevents overfitting, something that is
a major concern when the number of predictors exceeds the number of
observations.  Those wishing for a logistic regression model that mirrors
R’s <code>glm()</code> should use <code>statsmodels</code>’s <code>GLM</code>.</p>

</div>
</div>
</div>
 




					</div>

				</div>

			</div>

			<footer class="footer">

				<p>&copy; Copyright <YEAR>, Built using the minimal jupinx template.</p>

			</footer>

		</div>

		<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
		<script src="/_static/js/base.js"></script>

	</body>
</html>