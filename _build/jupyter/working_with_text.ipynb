{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='index-0'></a>\n",
    "\n",
    "<a id='working-with-text'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with text\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">This tutorial is available for interactive use\n",
    "with [IPython Notebook](http://ipython.org/notebook.html): <a href=Working with text.ipynb download>Working with text.ipynb</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a document-term matrix\n",
    "\n",
    "Word (or n-gram) frequencies are typical units of analysis when working with\n",
    "text collections.  It may come as a surprise that reducing a book to a list of\n",
    "word frequencies retains useful information, but practice has shown this to\n",
    "be the case. Treating texts as a list of word frequencies (a vector) also makes\n",
    "available a vast range of mathematical tools developed for [studying and\n",
    "manipulating vectors](http://en.wikipedia.org/wiki/Euclidean_vector#History).\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Turning texts into unordered lists (or “bags”) of words is easy in\n",
    "Python.  [Python Programming for the Humanities](http://fbkarsdorp.github.io/python-course/) includes a chapter entitled\n",
    "[Text Processing](http://nbviewer.ipython.org/urls/raw.github.com/fbkarsdorp/python-course/master/Chapter%203%20-%20Text%20Preprocessing.ipynb)\n",
    "that describes the steps in detail.\n",
    "\n",
    "This tutorial assumes some prior exposure to text analysis so we will gather\n",
    "word frequencies (or term frequencies) associated with texts into\n",
    "a document-term matrix using the [CountVectorizer](http://scikit-learn.sourceforge.net/dev/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "class from the [scikit-learn](http://scikit-learn.sourceforge.net/) package.\n",
    "(For those familiar with R and the [tm](http://cran.r-project.org/web/packages/tm/) package, this function performs\n",
    "the same operation as `DocumentTermMatrix` and takes recognizably similar\n",
    "arguments.)\n",
    "\n",
    "First we need to import the functions and classes we intend to use, along with\n",
    "the customary abbreviation for functions in the `numpy` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np  # a conventional alias\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the [CountVectorizer](http://scikit-learn.sourceforge.net/dev/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "class to create a document-term matrix. `CountVectorizer` is customizable. For\n",
    "example, a list of “stop words” can be specified with the `stop_words`\n",
    "parameter. Other important parameters include:\n",
    "\n",
    "- `lowercase` (default `True`) convert all text to lowercase before\n",
    "  tokenizing  \n",
    "- `min_df` (default `1`) remove terms from the vocabulary that occur in\n",
    "  fewer than `min_df` documents (in a large corpus this may be set to\n",
    "  `15` or higher to eliminate very rare words)  \n",
    "- `vocabulary` ignore words that do not appear in the provided list of words  \n",
    "- `strip_accents` remove accents  \n",
    "- `token_pattern` (default `u'(?u)\\b\\w\\w+\\b'`) regular expression\n",
    "  identifying tokens–by default words that consist of a single character\n",
    "  (e.g., ‘a’, ‘2’) are ignored, setting `token_pattern` to `'(?u)\\b\\w+\\b'`\n",
    "  will include these tokens  \n",
    "- `tokenizer` (default unused) use a custom function for tokenizing  \n",
    "\n",
    "\n",
    "For this example we will use texts by Jane Austen and Charlotte Brontë. These\n",
    "texts are available in [Datasets](datasets.ipynb#datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "filenames = ['data/austen-brontë/Austen_Emma.txt',\n",
    "             'data/austen-brontë/Austen_Pride.txt',\n",
    "             'data/austen-brontë/Austen_Sense.txt',\n",
    "             'data/austen-brontë/CBronte_Jane.txt',\n",
    "             'data/austen-brontë/CBronte_Professor.txt',\n",
    "             'data/austen-brontë/CBronte_Villette.txt']\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename')\n",
    "dtm = vectorizer.fit_transform(filenames)  # a sparse matrix\n",
    "vocab = vectorizer.get_feature_names()  # a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a document-term matrix and a vocabulary list. Before we can query\n",
    "the matrix and find out, for example, how many times the word ‘house’ occurs in\n",
    "*Emma* (the first text in `filenames`), we need to convert this matrix from\n",
    "its current format, a [sparse matrix](http://docs.scipy.org/doc/scipy/reference/sparse.html), into a normal NumPy\n",
    "array. We will also convert the Python list storing our vocabulary, `vocab`,\n",
    "into a NumPy array, as an array supports a greater variety of operations than\n",
    "a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# for reference, note the current class of `dtm`\n",
    "type(dtm)\n",
    "dtm = dtm.toarray()  # convert to a regular array\n",
    "vocab = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**\n",
    ">\n",
    ">A sparse matrix only records non-zero entries and is used to store\n",
    "matrices that contain a significant number of entries that are zero. To\n",
    "understand why this matters enough that `CountVectorizer` returns a sparse\n",
    "matrix by default, consider a 4000 by 50000 matrix of word frequencies that\n",
    "is 60% zeros. In Python an integer takes up four bytes, so using a sparse\n",
    "matrix saves almost 500M of memory, which is a considerable amount of\n",
    "computer memory in the 2010s. (Recall that Python objects such as arrays are stored in\n",
    "memory, not on disk). If you are working with a very large collection\n",
    "of texts, you may encounter memory errors after issuing the commands above.\n",
    "Provided your corpus is not truly massive, it may be advisable to locate\n",
    "a machine with a greater amount of memory. For example, these days it is possible to\n",
    "rent a machine with 64G of memory by the hour. Conducting experiments\n",
    "on a random subsample (small enough to fit into memory) is also recommended.\n",
    "\n",
    "With this preparatory work behind us, querying the document-term matrix is\n",
    "simple. For example, the following demonstrate two ways finding how many times\n",
    "the word ‘house’ occurs in the first text, *Emma*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# the first file, indexed by 0 in Python, is *Emma*\n",
    "filenames[0] == 'data/austen-brontë/Austen_Emma.txt'\n",
    "\n",
    "# use the standard Python list method index(...)\n",
    "# list(vocab) or vocab.tolist() will take vocab (an array) and return a list\n",
    "house_idx = list(vocab).index('house')\n",
    "dtm[0, house_idx]\n",
    "\n",
    "# using NumPy indexing will be more natural for many\n",
    "# in R this would be essentially the same, dtm[1, vocab == 'house']\n",
    "dtm[0, vocab == 'house']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although dtm is technically a NumPy array, I will keep referring to dtm as\n",
    "a matrix. Note that NumPy arrays do support matrix operations such as dot\n",
    "product. (If `X` and `Y` have compatible dimensions, `X.dot(Y)` is the\n",
    "matrix product $ XY $.)\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">NumPy does make available a [matrix](http://docs.scipy.org/doc/numpy/reference/generated/numpy.matrix.html)\n",
    "data structure which can be useful if you are doing lots of matrix\n",
    "operations such as matrix product, inverse, and so forth. In general,\n",
    "however, it is best to stick to NumPy arrays. In fact, if you are\n",
    "using Python 3.5 you can make use of the matrix multiplication operator `@`\n",
    "and dispense with any need for the `matrix` type.\n",
    "\n",
    "Just so we have a sense of what we have just created, here is a section of the\n",
    "document-term matrix for a handful of selected words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing texts\n",
    "\n",
    "Arranging our texts in a document-term matrix make available a range of\n",
    "exploratory procedures. For example, calculating a measure of similarity between\n",
    "texts becomes simple. Since each row of the document-term matrix is a sequence\n",
    "of a novel’s word frequencies, it is possible to put mathematical notions of\n",
    "similarity (or distance) between sequences of numbers in service of calculating\n",
    "the similarity (or distnace) between any two novels. One frequently used measure\n",
    "of distance between vectors (a measure easily converted into a measure of similarity) is [Euclidean\n",
    "distance](https://en.wikipedia.org/wiki/Euclidean_distance). The Euclidean\n",
    "distance between two vectors in the plane should be familiar from geometry, as\n",
    "it is the length of the hypotenuse that joins the two vectors. For instance,\n",
    "consider the Euclidean distance between the vectors $ \\vec{x} = (1, 3) $ and\n",
    "$ \\vec{y} = (4, 2) $. The distance between the two vectors is\n",
    "$ \\sqrt{(1-4)^2 + (3-2)^2} = \\sqrt{10} $.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Measures of distance can be converted into measures of similarity. If your\n",
    "measures of distance are all between zero and one, then a measure of\n",
    "similarity could be one minus the distance. (The inverse of the distance\n",
    "would also serve as a measure of similarity.)\n",
    "\n",
    "Distance between two vectors>**Note**\n",
    ">\n",
    ">More generally, given two vectors $ \\vec{x} $ and $ \\vec{y} $\n",
    "in $ p $-dimensional space,  the Euclidean distance between the two\n",
    "vectors is given by\n",
    "\n",
    "$ ||\\vec{x} - \\vec{y}|| = \\sqrt{\\sum_{i=1}^p (x_i - y_i)^2} $\n",
    "\n",
    "This concept of distance is not restricted to two dimensions. For example, it is\n",
    "not difficult to imagine the figure above translated into three dimensions. We can also persuade ourselves that the measure of distance extends to an arbitrary number of dimensions; for any two matched components in a pair of vectors (such as $ x_2 $ and $ y_2 $), differences increase the distance.\n",
    "\n",
    "Since two novels in our corpus now have an expression as vectors, we can\n",
    "calculate the Euclidean distance between them. We can do this by hand or we can\n",
    "avail ourselves of the `scikit-learn` function `euclidean_distances`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# \"by hand\"\n",
    "n, _ = dtm.shape\n",
    "dist = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        x, y = dtm[i, :], dtm[j, :]\n",
    "        dist[i, j] = np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "dist = euclidean_distances(dtm)\n",
    "\n",
    "np.round(dist, 1)\n",
    "# *Pride and Prejudice* is index 1 and *Jane Eyre* is index 3\n",
    "filenames[1] == 'data/austen-brontë/Austen_Pride.txt'\n",
    "filenames[3] == 'data/austen-brontë/CBronte_Jane.txt'\n",
    "\n",
    "# the distance between *Pride and Prejudice* and *Jane Eyre*\n",
    "dist[1, 3]\n",
    "\n",
    "# which is greater than the distance between *Jane Eyre* and *Villette* (index 5)\n",
    "dist[1, 3] > dist[3, 5]\n",
    "\n",
    "@suppress\n",
    "assert dist[1, 3] > dist[3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to use a measure of distance that takes into consideration the\n",
    "length of the novels (an excellent idea), we can calculate the [cosine\n",
    "similarity](http://www.gettingcirrius.com/2010/12/calculating-similarity-part-1-cosine.html)\n",
    "by importing `sklearn.metrics.pairwise.cosine_similarity` and use it in place\n",
    "of euclidean_distances.\n",
    "\n",
    "Keep in mind that cosine similarity is a measure of similarity (rather than\n",
    "distance) that ranges between 0 and 1 (as it is the cosine of the angle between\n",
    "the two vectors).  In order to get a measure of distance (or dissimilarity), we\n",
    "need to “flip” the measure so that a larger angle receives a larger value. The\n",
    "distance measure derived from cosine similarity is therefore one minus the\n",
    "cosine similarity between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(dtm)\n",
    "np.round(dist, 2)\n",
    "\n",
    "# the distance between *Pride and Prejudice* (index 1)\n",
    "# and *Jane Eyre* (index 3) is\n",
    "dist[1, 3]\n",
    "\n",
    "# which is greater than the distance between *Jane Eyre* and\n",
    "# *Villette* (index 5)\n",
    "dist[1, 3] > dist[3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those interested in doing the calculation for themselves can use the following\n",
    "steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "norms = np.sqrt(np.sum(dtm * dtm, axis=1, keepdims=True))  # multiplication between arrays is element-wise\n",
    "dtm_normed = dtm / norms\n",
    "similarities = np.dot(dtm_normed, dtm_normed.T)\n",
    "np.round(similarities, 2)\n",
    "# similarities between *Pride and Prejudice* and *Jane Eyre* is\n",
    "similarities[1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing distances\n",
    "\n",
    "It is often desirable to visualize the pairwise distances between our texts.\n",
    "A general approach to visualizing distances is to assign a point in a plane to\n",
    "each text, making sure that the distance between points is proportional to the\n",
    "pairwise distances we calculated. This kind of visualization is common enough\n",
    "that it has a name, “[multidimensional scaling](https://en.wikipedia.org/wiki/Multidimensional_scaling)” (MDS) and family of\n",
    "functions in `scikit-learn` (and R too, see `mdscale`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import os  # for os.path.basename\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "# short versions of filenames:\n",
    "# convert 'data/austen-brontë/Austen_Emma.txt' to 'Austen_Emma'\n",
    "names = [os.path.basename(fn).replace('.txt', '') for fn in filenames]\n",
    "# color-blind-friendly palette\n",
    "for x, y, name in zip(xs, ys, names):\n",
    "    color = 'orange' if \"Austen\" in name else 'skyblue'\n",
    "    plt.scatter(x, y, c=color)\n",
    "    plt.text(x, y, name)\n",
    "\n",
    "@suppress\n",
    "plt.tight_layout()\n",
    "\n",
    "@savefig plot_getting_started_cosine_mds.png width=8in\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do MDS in three dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# après Jeremy M. Stober, Tim Vieira\n",
    "# https://github.com/timvieira/viz/blob/master/mds.py\n",
    "\n",
    "mds = MDS(n_components=3, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(pos[:, 0], pos[:, 1], pos[:, 2])\n",
    "for x, y, z, s in zip(pos[:, 0], pos[:, 1], pos[:, 2], names):\n",
    "    ax.text(x, y, z, s)\n",
    "\n",
    "@savefig plot_getting_started_cosine_mds_3d.png width=7in\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering texts based on distance\n",
    "\n",
    "Clustering texts into discrete groups of similar texts is often a useful\n",
    "exploratory step. For example, a researcher may be wondering if certain textual\n",
    "features partition a collection of texts by author or by genre. Pairwise\n",
    "distances alone do not produce any kind of classification. To put a set of\n",
    "distance measurements to work in classification requires additional assumptions,\n",
    "such as a definition of a group or cluster.\n",
    "\n",
    "The ideas underlying the transition from distances to clusters are, for the most\n",
    "part, common sense. Any clustering of texts should result in texts that are\n",
    "closer to each other (in the distance matrix) residing in the same cluster.\n",
    "There are many ways of satisfying this requirement; there no unique clustering\n",
    "based on distances that is the “best”. One strategy for clustering in\n",
    "circulation is called [Ward’s method](https://en.wikipedia.org/wiki/Ward%27s_method). Rather than producing\n",
    "a single clustering, Ward’s method produces a hierarchy of clusterings, as we\n",
    "will see in a moment. All that Ward’s method requires is a set of pairwise\n",
    "distance measurements–such as those we calculated a moment ago.  Ward’s method\n",
    "produces a hierarchical clustering of texts via the following procedure:\n",
    "\n",
    "1. Start with each text in its own cluster  \n",
    "1. Until only a single cluster remains,  \n",
    "  - Find the closest clusters and merge them. The distance between two clusters\n",
    "    is the change in the sum of squared distances when they are merged.  \n",
    "1. Return a tree containing a record of cluster-merges.  \n",
    "\n",
    "\n",
    "The function [scipy.cluster.hierarchy.ward](http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html) performs\n",
    "this algorithm and returns a tree of cluster-merges. The hierarchy of clusters\n",
    "can be visualized using `scipy.cluster.hierarchy.dendrogram`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist)\n",
    "\n",
    "# match dendrogram to that returned by R's hclust()\n",
    "dendrogram(linkage_matrix, orientation=\"right\", labels=names)\n",
    "\n",
    "plt.tight_layout()  # fixes margins\n",
    "\n",
    "@savefig plot_getting_started_ward_dendrogram.png width=7in\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those familiar with R, the procedure is performed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide-output": false
   },
   "source": [
    "```r\n",
    "labels = c('Austen_Emma', 'Austen_Pride', 'Austen_Sense', 'CBronte_Jane',\n",
    "           'CBronte_Professor', 'CBronte_Villette')\n",
    "dtm_normed = dtm / rowSums(dtm)\n",
    "dist_matrix = dist(dtm_normed)\n",
    "tree = hclust(dist_matrix, method=\"ward\")\n",
    "plot(tree, labels=labels)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Find two different ways of determining the number of times the word\n",
    "  ‘situation’ appears in *Emma*. (Make sure the methods produce the same result.)  \n",
    "1. Working with the strings below as documents and using `CountVectorizer`\n",
    "  with the `input='content'` parameter, create a document-term matrix.\n",
    "  Apart from the `input` parameter, use the default settings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "text1 = \"Indeed, she had a rather kindly disposition.\"\n",
    "text2 = \"The real evils, indeed, of Emma's situation were the power of having rather too much her own way, and a disposition to think a little too well of herself;\"\n",
    "text3 = \"The Jaccard distance is a way of measuring the distance from one set to another set.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the document-term matrix just created, calculate the Euclidean\n",
    "  distance, [Jaccard distance](http://en.wikipedia.org/wiki/Jaccard_index),\n",
    "  and cosine distance between each pair of documents. Make sure to calculate\n",
    "  distance (rather than similarity). Are our intuitions about which texts are\n",
    "  most similar reflected in the measurements of distance?  \n",
    "\n",
    "\n",
    "*For solutions, view the source for this document.*"
   ]
  }
 ],
 "metadata": {
  "date": 1577189941.0232668,
  "filename": "working_with_text.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Working with text"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}